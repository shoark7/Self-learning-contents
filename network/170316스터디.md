#170316스터디

##커스텀 쉘 스크립트
C언어를 공부할때 귀찮은 표준 라이브러리 include와 main함수 뼈대를 자동으로 만들어 주는 스크립트를 작성하려고 했는데  
실패했다. 대신 텍스트 파일을 만들어 주는 c코드를 작성하는데 성공했다. 스크립트는 gcc의 긴 옵션을 정리해 주는 정도로 만족해야 할 것 같다.  
코드는 다음과 같다.  

```{.c}
#include <string.h>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/stat.h>
#include <sys/types.h>
#include <fcntl.h>
#include <errno.h>

/*
	static의 특징과 bss영역의 특징을 정확하게 꿰고 있어야 작성할 수 있는 프로그램이다.  
	이렇게 만들면 지역 변수의 특징을 가지는 전역 변수인 정적 변수 buf를 만들 수 있게 된다.  
	또한 프로그램의 런타임에 시동을 거는(?)과정에서 스타트업 코드가 bss영역을 0으로 초기화 한다는 점을 기억하면 
	효율적인 코딩이라는 사실도 알 수 있다.
 */

int main (int argc, char *argv[])
{
	// argv[1]이 파일 이름이다!
	// 이제 기능을 하나씩 추가하고, 에러처리를 보다 일반적인 형태로 만들어 보자.  
	static char buf[1024];
	int i, len;
	int fd, nr; 

	if (argc <= 1){ 
		fputs("1. 파일이름 2 ~ 라이브러리 이름)\n", stdout);
		exit(1);
	}   

	// 에러처리 필요 creat()대신 open()을 사용해도 좋다.  
	fd = creat(argv[1], 0664);
	if(fd == -1){
		perror("open");
		exit(1);
	}   
	
	// 버퍼에 필요한 문자열을 전부 집어넣고 시스템 콜 횟수를 최소화한다.
	for(i = 2; argv[i]; i++){
		strcat(buf, "#include <");
		strcat(buf, argv[i]);
		strcat(buf, ".h>\n");
	}   

	strcat(buf, "\nint main(void)\n{\n}");

	// 여기서 한번만 write()를 호출해서 성능감소를 최소화한다.  
	// 사실 이 부분이 에러처리를 확실하게 해줘야 하는 부분이다.  
	nr = write(fd, buf, strlen(buf));
	if(nr == -1){
		perror("write");
		exit(1);
	}   

	return 0;
}
```


##C포인터에 대해서
about_ptr 디렉토리를 참고하라.  


##가상 메모리 보충
먼저 중요한 용어들을 다시 정리하고 시작하자.  
(참고로 가상 메모리를 사용하지 않는 임베디드 제품에는 대신 *오버레이*라는 방식을 사용하여  
 작은 물리 메모리를 더 많이 쓸 수 있게 한다고 한다.)  

###프로세스 이미지(process image)  
이름 그대로 프로세스가 실행될 때 올라가야 하는 메모리 구조를 Process Image라고 한다.  
그 구성 요소로는, 초기화된 데이터 세그먼트, BSS영역, 스택 세그먼트, 힙 세그먼트,  
코드/텍스트 세그먼트 등이 있다. 이 외에도 커널 영역, 메모리 매핑된 파일,  
스레드에 배정된 스택 영역, 공유 라이브러리, 정적 라이브러리등이 있다.  


###섹터
블록 디바이스의 최소 접근 단위는 섹터라고 부른다.  
섹터는 디바이스의 물리적인 속성이다. 섹터는 2의 승수로 나타내며 512바이트가 가장 일반적이다.  
블록 디바이스는 섹터보다 더 작은 데이터 단위를 전송하거나 더 작은 데이터 단위에 접근하지 못한다.  
모든 입출력은 섹터 하나 이상을 기준으로 일어난다. 
따라서 1섹터에 여러개의 파일을 저장하는 것은 **불가능**하다.  


그리고 운영 체제는 파일의 실제 크기와 상관없이 파일이 여러 섹터를 차지할 수 있도록 설계되었다.  
전체 섹터를 딱 나누어 떨어지게 차지하지 않는 파일은 마지막 섹터의 나머지 부분을 0으로 채운다.  
실제로 운영체제는 **데이터 블록**상에서 동작하며, 하나 이상의 섹터를 포괄할/걸쳐 있을 수 있다.  


이는 추상 단위/속성이 아니며 다분히 물리적인 속성을 강조한 단어다.  
###블록
물리적인 최소 접근 단위가 섹터라면 파일시스템에서 논리적인 최소 접근 단위는 블록이다.  
블록은 파일시스템을 추상화한 개념으로 보통 섹터 크기의 2의 승수이다. 
블록은 일반적으로 섹터보다 더 크지만(물론 2^0을 곱하여 같은 크기를 가질 수도 있다.)  
페이지 크기(하드웨어 구성 요소인 메모리 관리 유닛에서 지정한 최소 단위 ->  
이는 커널의 인위적인 제한으로 언젠가는 사라질 수도 있다.) 보다는 작다.  
보통 블록 크기는 512바이트, 1024바이트, 4096바이트다.  


앞서 말했듯이 블록은 파일 시스템의 최소 저장 단위를 나타내는 추상 개념이다.  
실제로 커널 내부를 살펴보면 모든 파일 시스템 연산은 블록 단위로 일어난다.  
*커널 내부에서 블록은 입출력에 관한 공통어라고 할 수 있다.*  
따라서 모든 입출력 연산은 블록 크기의 정수배에 맞춰서 일어난다.  


블로킹(blocking)은 블록에 데이터를 저장하는 과정을 말한다.  
디블로킹(deblocking)은 블록에서 데이터를 추출하는 과정을 말한다.  


*커널 내부적인 추상화 단위와 응용 애플리케이션의 추상화 단위의 차이점과 활용에 대해서 서술이 필요하다.*


블럭된 데이터는 보통 데이터 버퍼에 저장된다. 그리고 하나의 블록(버퍼?) 전체가 한번에 읽거나 쓰게 된다.  
따라서 블로킹은 오버헤드를 줄이고 데이터 스트림의 처리 속도를 높인다.  
파일 크기가 블록크기의 정수배를 이룬다면 가장 이상적으로 하드웨어를 사용할 수 있지만,  
아닌 경우가 있기 때문에 내부적으로 디스크 단편화가 생기고 이로 인해서 공간 비효율이 생긴다.  


또한 앞서 말했듯 블럭크기는 대체로 512바이트이거나 그 정수배를 취하는데,  
이는 역사적인 이유에 기인한다. 전통적으로 HDD의 Disk Sector가 512바이트이기 때문이다.  
여기에 더해 대부분의 파일 시스템은 블록 디바이스(커널에서 블록 단위로 입출력을 수행한다.)를  
기반으로 구현되어 있다.  
###블록 디바이스
유닉스 계열 운영체제에선 장치 파일이나 특수 파일은 마치 일반 파일인 것처럼  
파일 시스템에 표현되는 장치 드라이버의 인터페이스 역할을 맡고 있는 객체다.  
그 중에서도 눈에 띄는 것이 블록 디바이스 파일이다.  


블록 디바이스는 보통 저장장치를 말하는데, 하드 디스크, 플로피 드라이브, CD-ROM드라이브,  
플래시 메모리는 모두 블록 디바이스에 해당한다.  
블록 디바이스 역시 디바이스 파일을 통해서 접근한다.  


블록 특수 파일이나 블록 디바이스는 하드웨어 장치에 대한 버퍼링된 접근을 제공하고  
세부적인 사항을 추상화 한다.  
문자 장치(문자 디바이스 파일은 사용자가 키보드에 입력할때마다 데이터가  
enqueue되는 일종의 선형 큐라고 생각하면 된다.)와는 달리 블록 디바이스는 항상 프로그래머가  
모든 크기의 블록 (단일 문자 / 바이트 포함) 및 정렬을 읽거나 쓸 수 있다.  
(블록 디바이스의 특징은 커널이 블록 단위로 임의의 주소에 접근 가능하다는 점이다.)    


자세한 사항은 버퍼 입출력에 대해서 공부하면 더 많은 것을 알 수 있게 될 것이다.  
###페이지
페이지는 메모리 관리 유닛에서 사용하는 최소 단위다.  
즉 페이지는 별도의 접근 권한과 동작 방식을 따르는 가장 작은 메모리라고 할 수 있다.  
페이지는 메모리 맵핑을 구성하는 블록이자 프로세스 주소 공간을 구성하는 블록이다.


좀 더 자세히 말하자면 페이지는 1개의 페이지 테이블 엔트리로 간주되는  
가상 메모리의 고정된 길이의 연속된 블록이다.  
주 메모리와 하드 디스크 드라이브와 같은 보조 저장소 사이의 페이지 이동, 전송 등  
페이지를 관리하는 기법 또는 행위를 페이징, 또는 스와핑이라고 한다.  


페이지를 보다 자세하게 이해하려면 가상 주소와 물리 주소,  
그리고 가상 메모리에 대해서 공부하면 보다 자세하게 알 수 있다.  

###페이지 프레임
페이지 프레임은 메모리 페이지가 운영체제에 의해 매핑되는  
물리적 메모리의 가장 작은 고정된 길이의 연속된 블록들을 뜻한다.  

###페이지 테이블 엔트리
프로세스가 메모리의 데이터에 대한 액세스를 요구할 때,  
프로세스가 제공한 가상 주소를 해당 데이터가 저장된 실제 메모리에 매핑하는 것은 운영체제의 책임이다.  
페이지 테이블은 운영 체제가 가상 주소의 매핑을 실제 주소에 저장하는 곳으로  
각 매핑은 페이지 테이블 항목(PTE)라고 한다.  

###페이지 테이블
페이지 테이블은 가상 주소와 실제 주소 사이의 매핑을 저장하기 위해  
컴퓨터 운영체제의 가상 메모리 시스템에서 사용하는 데이터 구조다.  
가상 주소는 운영체제와 그 구성 요소에서 사용하는 반면,  
물리적 주소는 하드웨어 또는 RAM서브 시스템에 의해 사용한다.  

###페이징, 스와핑
페이징 기법(paging)은 컴퓨터가 메인 메모리에서 사용하기 위해 2차 기억 장치로부터  
데이터를 저장하고 검색하는 메모리 관리 기법이다.  
흔히 '~부족한 물리 메모리의 한계를 극복하기 위해'라고 말하는데,  
틀린말은 아니지만 이것만 말한다면 부족한 대답이다.  


페이징은 시스템이 돌아가는 동안 끊임없이 일어나며, 
페이지를 통한 가상 메모리를 사용하는 시스템이라면  
전혀 메모리가 부족하지 않은 상황에서도 페이징은 일어난다.  


단지, 사용자의 물리 메모리가 부족해지는 상황이 온다면 보조 기억장치에  
미리 만들어 둔 페이징 파일, 스왑 영역등의 용량을 크게 증가시켜서 활용하게 되며,  
페이지 교체(Page Replacement)가 많이 발생하게 된다.
(페이지 교체란 페이지를 통해서 실제 물리 메모리를 올리고 내리고 하는 일련의 작업을 말한다.)  
이 페이지 교체의 정도가 심해지면 프로세스가 맡은 일을 수행하는 데  
프로세서 자원을 사용하는 것 보다 페이지 부재(Page fault)에 맞춰서 페이지를 교체하는 데  
더 많은 자원을 소비하는 것을 Thrashing(쓰레싱)이라고 한다.
한 마디로 시스템이 신나게 뻘짓을 하고 있는 중이라고 생각하면 된다.  


쓰레싱에 관해서는 아래 절에서 자세한 내용을 확인 할 수 있다.  
###페이징 파일, 스와핑 파일, 스왑 영역
윈도우에서는 페이징 파일이라고 부르는 파일이 따로 배정되고,(동적으로 크기도 조정가능)  
리눅스에서는 스왑 영역이라고 하는 파티션을 따로 배정해서 활용한다.(동적 크기 조정 불가능)  
해당 파일과 파티션이 하는 역할은 바로 페이징을 수행하는데 우선순위에서 밀려난 페이지만 저장할 수도 있고,  
시스템에 따라서 코드와 데이터 전체, 즉 프로세스 이미지 전체를 스왑 공간에 가지고 있게 할 수도 있다.  
따라서 스왑 공간의 크기를 물리 메모리의 크기, 가상 메모리의 크기, 가상 메모리가  
사용되는 방식에 따라 적게는 수 MB에서 많게는 수 GB까지이다.  


###세그먼트
세그먼트란 컴퓨터의 주 메모리를 세그먼트 또는 섹션이라고 하는 논리적인 단위로 나누는 것이다.  
세그멘테이션을 사용하는 시스템에서, 메모리 위치에 대한 메모리 참조는  
세그먼트 및 그 세그먼트 내의 오프셋(변위\메모리 위치)를 식별하는 값을 포함한다.  


세그먼트는 바이트 단위로 그 크기를 정의하기도 하고  
페이지와 함께 구현되어 페이지 단위로 관리될 수도 있다.  
또한 고정되어 있지 않은 크기를 가지기에, 어떤 세그먼트인지에 따라서 굉장히 작은 크기를 가질수도 있고,  
반대로 굉장히 큰 크기를 가질수 있다. 물론 각 세그먼트의 크기는 같지 않다.  


이런 특징을 가지고 있는 세그먼트는 페이지와 함께 사용되지 않으면 외부단편화가 생길 가능성이 크다.  
따라서 대부분의 현대 운영체제에서는 페이지와 세그먼트를 함께 구현하여 메모리를 관리한다.  
다만 페이지를 사용하면 *가상 메모리의 고정된 연속 블록의 묶음*이라는 정의에 맞게,  
**내부적으로 단편화**가 생길 수 있지만, 충분히 감당할 수 있는 수준이다.  
### 가상 주소 공간, 물리 주소 공간
물리 주소 공간은 알기 쉽다. 말 그대로 실제 시스템이 사용하는 램의  
주소 공간을 일컫는 말이다. 


가상 주소 공간은 가상 메모리 기법을 사용하는 시스템에서 **각 프로세스**에게 제공되는 주소 공간으로,  
프로세스 관점에서 사용할 수 있는 일련의 가상 주소 집합이다.  
가상 주소의 범위는 일반적으로 낮은 주소에서 시작하여, 
컴퓨터의 명령어 세트 아키텍처에서 허용하고 운영체제의 포인터 크기 구현에 의해 지원되는  
최대 주소까지 확장할 수 있다.(예 -> 32비트 4GB, 64비트 16EB)  


가상 주소 공간을 사용해서 얻는 이득은 여러가지가 있는데,  
가장 큰 이득은 프로세스 고립(Process isolation)을 통한 **보안**이다.  
나머지는 아래에서 하나씩 차근차근 알아볼 것이다.  


참고로 찾다 보면 선형 주소, 논리 주소라는 용어도 나올 것이다.  
이는 문맥에 따라서 조금씩 다른 뜻을 나타낼 수 있으며, 아키텍처에 따라서도 다른 뜻을 나타낼 수도 있다.  
하지만 확실이 알아둬야 할 것은 선형 주소와 논리 주소는 모두 가상 주소와 깊은 관련이 있다는 점이다.  
경우에 따라서는 거의 같은 뜻을 나타내지만 미묘하게 다르거나 다른 뜻을 나타낼 수도 있다.  
따라서 X86-64아키텍처와 깊은 연관성을 보이는 단어-정도로 알고 있으면 될 것 같다.  
### MMU, TLB
PMMU(paged Memory management unit)이라고도 불리는 메모리 관리 장치(Memory Management Unit, 줄여서 MMU)는 CPU가 메모리에 접근하는 것을 관리하는 컴퓨터 하드웨어 부품이다.  
가상 메모리 주소를 실제 메모리 주소로 변환하며, 메모리 보호, 캐시 관리, 버스 중재 등의 역할을 담당하며 간단한 8비트 아키텍처에서는 뱅크 스위칭을 담당하기도 한다.  


참고로 뱅크 스위치가 뜻하는 바는 다음과 같다.  
뱅크 전환(bank switching)은 마이크로프로세서의 주소 공간보다 많은 메모리를 활용하기 위해 개발된 기술이다.  
주로 8비트 마이크로프로세서에서 사용되었으며, 거의 대부분의 8비트 마이크로프로세서가 16비트 주소 공간이므로 216 = 65536 매모리 공간을 갖는다. 
이것보다 많은 메모리를 확장하려면 뱅크스위칭을 사용한다. 그러나 주소공간 내에서 액세스가 되어야 하므로 동시에 64 kB보다 많은 공간을 식별할 수 없다. 
따라서 사용하는 뱅크를 설정하고 해당공간만을 액세스하다가, 뱅크를 바꾸어 다른 공간을 활용한다.  
같은 주소공간의 뱅크를 스위칭하는 방법은 논리회로의 주소 디코더에 의한 설정에 의해 결정된다.  
같은 주소공간의 여러개의 뱅크 중에 선택된 뱅크만이 액세스 되도록 하고 선택되지 않은 뱅크는 데이터를 유지만 하도록 동작 한다  


최신 아키텍처에서 MMU는 가상 주소공간을 2N비트 크기의 페이지들로 나눈다. 그 가운데 일부 페이지는 실제 메모리 주소의 한 페이지에 대응되는데,  
대부분의 경우 가상 주소공간은 실제 메모리의 주소공간보다 크기 때문에 모든 페이지가 실제 메모리에 대응되는 것은 아니다.  
CPU가 가상 메모리 주소를 MMU에 넘겨주면 MMU는 그 주소를 받아 뒤쪽의 N비트는 바꾸지 않고 앞쪽의 나머지 비트를 그에 해당하는 실제 메모리 주소로 바꾼다.  
이때 가상 메모리 주소와 실제 메모리 주소 사이의 변환을 위해 MMU는 변환 참조 버퍼(Translation Lookaside Buffer, TLB)라는 고속의 보조기억장치를 참조한다.  
이 보조기억장치에 원하는 변환 정보가 없을 때는 더 느린 다른 방법으로 페이지 변환 정보를 얻어오는데, 이 페이지 변환 정보가 담겨 있는 자료구조를 페이지 테이블(Page Table)이라 한다.  
페이지 테이블의 동작은 아키텍처와 운영체제에 따라 서로 다르다.  

###TLB
변환 색인 버퍼(Translation Lookaside Buffer, TLB)는 가상 메모리 주소를 물리적인 주소로 변환하는 속도를 높이기 위해 사용되는 캐시로, 약칭은 TLB이다.  
TLB는 최근에 일어난 가상 메모리 주소와 물리 주소의 변환 테이블을 저장하기 때문에 일종의 주소 변환 캐시라고 할 수 있다.  
TLB는 CPU와 CPU 캐시 사이, CPU 캐시와 메인 메모리 사이 등 여러가지 다른 레벨의 캐시들 사이에서 주소를 변환하는데 사용할 수 있다.  
현재 모든 데스크탑 및 서버용 프로세서는 하나 또는 그 이상의 TLB를 메모리 관리 하드웨어에 가지고 있다.  
페이지 단위나 세그먼트 단위로 처리하는 가상 메모리를 사용하는 거의 모든 하드웨어는 TLB를 사용한다.  
CPU는 1차적으로 TLB에 접근하여 원하는 페이지가 존재하는지 탐색하고, TLB에 존재하지 않을 경우 MMU의 페이지 테이블을 참조한다.  

###요구 페이징
요구 페이징(demand paging)은 가상 메모리의 구현 방식이며  
세그멘테이션 방식으로 구현 될 수도 있으나,  
대개는 하나의 세그먼트가 다시 여러 페이지로 나뉘는 페이지된 세그멘테이션 기법을 사용한다.  


이를 프로세스 내의 개별 페이지 수준에서 관리해주는 것을 페이저(pager)라고 한다.
요구 페이징 기법은 어떤 점에서는 스와핑(swapping)과 비슷하지만,
요구 페이징과 관련해서는 스와핑, 스와퍼보다는 페이저라는 용어를 사용한다.  


페이저는 swap in시에 프로세스가 다시 swap out되기 전에  
실제로 사용될 페이지들이 어떤 것인지 추측한다.  
따라서 페이저는 프로세스 전체를 스왑 인 하는 대신에 실제 필요한 페이지들만 메모리로 읽어 온다.
이는 낭비를 줄일 수 있는 효과적인 방법이다.  


이 방법을 사용하려면 약간의 하드웨어 지원이 필요하고, 
유효-무효 비트기법이 여기서 사용될 수 있다. 
페이지 테이블 내의 페이지의 유효-무효 비트가 켜저(1)있다면 메모리 내부에 있는것이고,  
꺼져(0)있다면 가상 주소 공간 상에 정의되어 있지 않거나,  
유효하지만 디스크(보조 기억장치)에 존재한다는 것을 의미한다.  


이 때 프로세스가 꺼져있는 비트를 가진 페이지에 접근하려고 하면  
페이지 부재 트랩(page-fault trap)을 발생시킨다. 처리 과정은 그림을 보라. 
요구 페이징을 극단적으로 사용하면 메모리에 페이지가 하나도 올라오지 않은 상태에서도  
프로세스를 실행시킬 수 있다. 이런 방식을 *순수 요구 페이징(pure demand paging)*이라고 한다.


그러면 여기서 이런 의문을 가질 수도 있다. "요구 페이징은 성능 저하를 일으키지 않을까요?"
그러나 성능 저하는 생각보다 매우 작다. 모든 프로그램은 참조의 지역성(locality of reference)를 가진다.  
프로그램은 한동안 특정할 수 있는 작은 부분만 집중적으로 참조하는데  
이러한 성질 때문에 요구 페이징은 만족할 성능을 보인다.  
(그러나 구현이 쉬운 것이 결코 아니다.) 


스왑 인:  
보조기억장치에 있는 데이터를 메모리로 읽어들이는 행위  


스왑 아웃:
메모리에 있는 데이터를 보조기억장치에 쓰는 행위


스와핑:
위와 같이 보조기억장치와 주 메모리 사이에서 데이터를 주고받는 행위를 총칭하는 단어  

###페이지 교체 + 교체 알고리즘
페이지 부재에 대해서 말하지 않은 것이 있다.  
각 페이지는 처음 그 페이지가 접근 될 때 한번만 페이지 부재가 발생하는 것처럼 말했는데,  
이런 설명이 정확한 것이 아니다. 만약 페이지를 전체 10개 사용하는 프로그램이 
실제로는 5개의 페이지만 사용한다고 해보자, 하나의 프로그램에서 비교적 많은 프로세스가 실행 가능한 것은 맞지만,  
무한하게 실행 가능한 것은 아니다.(가상 메모리도 유한하다.)  


여기에 더해서 시스템 메모리는 프로그램 페이지만 저장하는 용도로만 사용되는 것이 아니다.  
입출력을 위한 버퍼도 상당한 양의 메모리를 사용하고, 이는 메모리 할당 알고리즘의 부담을 키운다.  
어느 정도의 메모리를 입출력 용도로 할당하고,  
또 어느 정도로 프로그램에 할당할 것인지 결정하는 것은 매우 중요한 문제다.  


여기서 운영체제는 몇 가지 선택을 할 수 있는데, 일단 사용자 프로세스를 종료할 수 있다.  
하지만 요구 페이징은 운영체제가 더 효율적인 사용자 경험을 위해서 선택한 방법이므로,  
위와 같은 선택은 가장 좋은 선택이라고 보기 어렵다.  
또한 사용자가 운영체제가 페이징 시스템에서 실행되고 있음을 알아서도 안된다.
대신 운영체제는 프로세스 하나를 swap out하여 해당 프로세스의 프레임들을 해제하고,
다중 프로그래밍[3] 정도를 낮출 수 있다. 어떤 환경에서 이 선택은 훌륭한 것이다.  


즉, 중요도가 떨어지는 페이지는 swap out하고 더 중요한 페이지를 디스크에서 swap in한다.
이를 페이지 교체(page replacement)라고 한다. 페이지 교체 과정에 대해서는 그림을 참고하라.  


페이지 교체는 요구 페이징의 기본이다. 이를 통해 논리적 메모리와 물리 메모리간의 분리가 완성된다.  
요구 페이징을 하지 않더라도 사용자 주소가 물리 주소로 사상되어 서로 다른 주소 집합을 가질 수 있다.  
그러나 이런 방식으로 구현하면 프로세스의 모든 페이지는 물리 메모리에 존재해야만 한다.  
요구 페이징은 논리 주소 공간의 크기가 물리 메모리에 의한 제약에서 벗어나도록 해준다.  
어떤 프로세스가 20개의 페이지로 이루어져 있으면, 10 프레임만 가지고도  
요구 페이징과 필요할 때 마다 빈 페이지를 찾아주는 교체 정책을 이용해 이 프로세스를 실행 할 수 있다.  
교체 되려는 페이지가 변경이 된 경우에는 디스크에 복사된다.  
나중에 이 페이지를 참조하게 되면 페이지 부재가 발생한다.  
그 때 페이지는 메모리로 다시 돌아오게 되며, 아마도 프로세스의 다른 페이지를 교체하게 될 것이다.  


요구 페이징 시스템은 두 가지 중요한 문제를 해결해야 하는데, 
**프레임 할당(frame allocation)알고리즘** 과 **페이지 교체(page replacement)알고리즘**이다.  
여러 프로세스가 있다면, 각 프로세스에 얼마나 많은 프레임을 할당해야 할 지 결정해야 한다.  
또한 페이지 교체가 필요할 때 마다 어떤 페이지를 교체해야 할지 결정해야 한다.  
디스크 입출력이 매우 많은 비용을 요구하는 것을 생각해 보면,  
이런 문제를 해결할 수 있는 적절한 알고리즘을 설계하는 것은 중요한 일이다.  
요구 페이징 방법은 조금만 개선해도 시스템의 전체 성능이 크게 향상 될 수 있다.  
이제 가장 유명한 페이지 교체 알고리즘 부터 하나씩 알아보자.  

1. FIFO 페이지 교체
가장 간단한 페이지 교체 알고리짐은 FIFO(first-in first-out)알고리즘이다.  
FIFO 교체 알고리즘은 어떤 페이지를 교체해야 할 때, 메모리에 올라온지 가장 오래된 페이지를 내쫓는다.  
페이지가 올라온 시간을 페이지 마다 기록하는 방법도 좋고,  
아니면 페이지들이 올라온 순서로 큐를(FIFO Queue) 만들어 가지고 있어도 된다.  


이 교체 알고리즘은 이해하기도 쉽고, 프로그램 하기도 쉽다.  
하지만 성능이 항상 좋지는 않다. 교체된 페이지가 더이상 사용되지 않는 초기화 모듈일 수도 있지만,  
반대로 교체된 페이지가 초기화 된 뒤 계속해서 자주 사용하는 변수를 포함하고 있을 수도 있다.  


실제로 이 알고리즘을 채택한 뒤 페이지 부재율을 관찰해 보면,
프레임을 더 줄수록 오히려 페이지 부재율이 증가한다는 사실을 알 수 있다.  
이런 결과는 상식 밖의 것이면, 이러한 현상을 **Belady의 모순**이라고 한다.  

2. 최적 페이지 교체
FIFO가 Belady의 모순으로 인해 별로 좋은 알고리즘이 아님이 밝혀지자,  
연구자들은 최적 교체 정책을 탐색하기 시작했다. 결과적으로 그런 정책은 존재 했고 OPT 또는 MIN으로 불렸다.  
이 정책을 요약하면 다음과 같다.  


**앞으로 가장 오랫동안 사용되지 않을 페이지를 찾아 교체하라**


실제로 (구현되었다고 가정하고)FIFO에 비해서 훨씬 좋은 성능을 보인다.
하지만 안타깝게도 이 알고리즘의 실제 구현은 어렵다.  
이 방식은 프로세스가 앞으로 메모리를 어떻게 참조할 것인지를 미리 알아야하기 때문이다.  
이 알고리즘이 사용되는 경우는 주로 비교 연구 목적을 위해서 사용한다.  

3. LRU(Least recently used \ 최근 최소 사용) 페이지 교체
최적 알고리즘이 불가능 하다면 최적 알고리즘의 근사 알고리즘은 가능할 것이다.  
FIFO와 OPT알고리즘의 결정적인 차이는 FIFO알고리즘이 페이지가 메모리로 들어온 시간을 이용하는데 비해,  
OPT알고리즘은 페이지가 사용될 시간을 이용한다는 것이다.  
최근의 과거를 가까운 미래의 근사치로 본다면, 가장 오랜 기간 동안 사용되지 않은 페이지를 교체할 수 있다.  
이 기법이 least-recently-used(LRU)알고리즘이다.  


이 기법은 각 페이지마다 마지막 사용 시간을 유지한다.  
페이지 교체시에 LRU는 가장 오랫동안 사용되지 않은 페이지를 선택한다.  
이 정책은 미래 대신 과거 시간에 대해 적용한 최적 교체 정책으로 생각할 수 있다.  


성능은 최적 교체 알고리즘보다는 못하지만, FIFO보다는 좋다.  
따라서 LRU정책은 페이지 교체 알고리즘으로 자주 사용되며 좋은 알고리즘으로 인정받고 있다.  
문제는 *어떻게* 이 알고리즘을 구현하느냐 하는 것이다.  
LRU 페이지 교체 알고리즘은 하드웨어의 지원이 필요하다.  
페이지들을 최근 사용된 시간 순서로 파악하는 것을 하드웨어가 지원해 주어야 한다.  
두 가지 구현 방법이 가능하다.  

* 계수기(Counter)

* 스택(Stack)

하지만 양쪽 LRU 구현 방법 모두 반드시 표준적인 TLB 레지스터 이상의 하드웨어 지원이 있어야 한다.  
계수기 값이나 스택을 갱신하는 일을 메모리 참조 때마다 수행되어야 한다.  
이정도 메모리 관리 오버헤드를 감당할 수 있는 시스템은 거의 없다.  


참고:  
LRU는 운영체제에서 페이지 교체에만 사용하는 알고리즘이 아니다.  
필요에 따라서 응용 프로그램 영역에서 충분히 사용할 수도 있다.  
4. LRU 근사 페이지 교체
LRU 페이지 교체 지원을 충분히 할 수 있는 하드웨어는 거의 없다.  
그러나 많은 시스템은 **참조 비트**(reference bit)의 형태로 어느 정도의 지원은 하고 있다.
처음에는 모든 참조 비트는 운영체제에 의해 0으로 채워진다.  
페이지 참조가 있을 때마다(페이지 내의 어떤 바이트라도 읽거나 쓸 때) 하드웨어가 1로 세팅한다.  
어느 정도 지나면 페이지 사용의 *순서*는 모르지만, 어떤 페이지가 그동안 사용되었고,  
어떤 페이지가 한번도 사용되지 않았는지를 알 수 있다.  
이런 부분적인 정보가 많은 LRU 근사 알고리즘의 기본이 된다.  

* 부가적 참조 비트 알고리즘
일정한 간격마다 참조 비트들을 기록함으로써 추가적인 선후 관계 정보를 얻을 수 있다.  
각 페이지에 대해 8비트의 참조 비트를 할당한다.  
일정한 시간 간격마다, 예를 들면 매 100ms마다, 타이머 인터럽트(timer interrupt)를 걸어서  
운영체제가 참조 비트를 8비트 정보의 최상위 비트로 이동시키고,  
나머지 비트들은 하나씩 오른쪽으로 이동시킨다.  
8비트 시프트 레지스터(shift register)는 가장 최근 8구간 동안의 그 페이지의 사용 기록을 담고 있다.  


예를 들어 시프트 레지스터 값이 00000000이라면 페이지를 8번의 구간동안 한번도 사용하지 않았다는 뜻이고,  
각 구간 마다 최소한 한번 이상 사용된 페이지는 11111111의 시프트 레지스터 값을 가진다.  
이 8비트 값을 정수로 생각하면 가장 작은 수를 갖는 페이지가 LRU페이지가 되고 이를 교체할 수 있다.  
가장 작은 값을 갖는 페이지 모두를 교체할 수도 있고 그들 사이에서 FIFO방식으로 하나를 선택 할 수도 있다.  


물론 사용하는 비트 수(시프트 레지스터의 크기)는 달라질 수 있고, (하드웨어의 지원 여부에 따라)
갱신을 가장 빠르게 하기 위한 크기가 선택된다.  
극단적인 경우 크기는 0(참조 비트 이외에 없음)될 수 있고, 이 경우 참조 비트만이 남게 된다.  
이 알고리즘을 **2차 기회 알고리즘**이라고 부른다.

* 2차 기회 알고리즘
2차 기회 알고리즘의 기본은 FIFO 교체 알고리즘이다.  
그러나 페이지가 선택될 때마다 참조 비트를 확인한다.  
참조 비트가 0이면 페이지를 교체하고 1이면 다시 한 번 기회를 주고 다음 FIFO페이지로 넘어간다.  
한 번 기회를 받게 되면 참조 비트는 해제되고 도착 시간이 현재 시간으로 재설정된다.  
이에 따라 그 페이지는 다른 모든 페이지들이 교체 될 때까지 (또는 기회를 받을 때 까지) 교체되지 않는다.  
따라서 참조 비트가 계속 설정되어 있을 정도로 자주 사용되는 페이지는 전혀 교체되지 않을 것이다.  


2차 기회 알고리즘을 구현하는 하나의 방법은 순환 큐(circular queue)를 이용하는 것이다.  
이 큐에는 포인터가 있어서 다음에 교체될 페이지를 가리킨다.  
희생될 페이지가 필요하다면, 포인터는 0값의 참조 비트를 가진 페이지를 발견할 때 까지 큐를 뒤진다.  
포인터가 돌아가면서 참조 비트 값들이 1인것은 0으로 바꾼다.  
희생될 페이지가 발견되면, 그 페이지는 교체되고 새로운 페이지는 순환 큐의 해당 위치에 삽입한다.  
최악의 경우, 모든 비트가 1값을 가지고 있었다면 포인터는 큐를 완전히 한바퀴 돈다.(그러면서 0값으로 세팅)  
그러고 나면 모든 페이지는 기회가 주어지게 된다. 두 번째 돌 때에는 사실상 FIFO와 같은 것이 된다.  


* 개선된 2차 기회 알고리즘
참조 비트와 변경 비트를 사용하면 더 개선시킬 수 있다.  
###COW(Copy on write - 쓰기 시 복사)
앞에서 페이지를 공유 함으로써 프로세스를 빠르게 생성할 수 있다는 요지의 말을 했는데,  
이번에는 그 부분에 대해서 알아보자.  
알다시피 fork()시스템 콜은 부모 프로세스와 똑같은 자식 프로세스를 만들어준다.  
하지만 자식 프로세스는 곧바로 exec()을 호출해서 부모로부터 복사해온 페이지들을 다 쓸모없게 만든다.  
그래서 fork()할 때 부모 프로세스의 페이지를 몽땅 복사해오는 대신에 쓰기 시 복사(COW)방식을 사용할 수 있다.  


이 방식은 자식 프로세스가 시작할 때 부모의 페이지를 당분간 함께 사용하도록 한다.  
이 때 공유되는 페이지를 쓰기 시 복사(copy-on-write)페이지라고 표시한다.  
"둘 중 한 프로세스가 공유 중인 페이지에 쓸 때 그 페이지의 복사본이 만들어 진다"는 의미이다.  
이렇게 되면, 자식 프로세스는 수정이 일어나는 페이지만 복사하여,  
수정되지 않는 페이지들에 대해서는 계속 공유 될 수 있게 한다.  


###메모리 사상 파일(Memory Mapped file)
open(), read(), write() 시스템 호출을 사용하여 디스크에 있는 파일을 순차적으로 읽는다고 생각해보자.  
이러한 방식을 사용하면 파일이 매번 액세스될 때마다 시스템 호출을 해야하고 디스크를 접근해야 한다.  
이와 같이 하는 대신 디스크 입출력을(지금까지 설명한 가상 메모리 기법을 사용하여)메모리 참조 방식으로 대신할 수도 있다.  
이러한 "메모리 사상(memory mapping)"이라고 불리는 접근 방식은 프로세스 가상 주소 공간 중 일부를 관련된 파일에 할애하는 것을 말한다.  
앞으로 보겠지만 이 방식은 현저하게 성능을 향상시킨다.  


파일의 메모리 사상은 프로세스의 페이지 중 일부분을 디스크에 있는 파일의 블록에 사상함으로써 이루어진다.  
첫 번째 접근은 일반적인 요구 페이징 과정에 따라 페이지 부재를 발생 시킨다.  
이때 그 파일 내용 중 페이지 크기만늠의 해당 부분이 파일 시스템으로부터 메모리 페이지로 읽혀 들어오게 된다.  
(시스템에 따라서는 이와 같이 읽어 들일 때 페이지 크기보다 더 큰 단위로 읽혀 들어오게 된다.)  
그 이후의 파일 read/write는 일반적인 메모리 액세스와 같이 처리된다.  
read()와 write()를 사용하는 오버헤드 없이 파일을 메모리를 통하여 조작하면 단순하고 더 빠르게 파일을 접근하고 사용할 수 있다.  


이처럼 메모리에 사상된 파일에 대한 write는 디스크에 즉시(동기화되어)write되지 않을 수도 있음을 주의한다.  
시스템에 따라서는 메모리에 사상된 파일들은 운영체제가 주기적으로 살펴보고 이때 발견된 변경된 페이지들을 물리적인 파일에 반영하기도 한다.  
프로세스가 close하면 그때까지 write되었던 모든 내용이 모두 디스크로 write한 후 가상 메모리에서 제거된다.  


어떤 운영체제는 특정 시스템 호출을 통해서만 메모리 사상 파일 처리를 해주고   
일반적으로는 표준 파일 관련 시스템 호출을 통해 파일에 접근하도록 한다.  
반면 어떤 운영체제는 메모리 사상 파일을 표준으로 삼기도 한다.  
Solaris의 경우 mmap()이라는 시스템 호출로 지정을 해야 
그 파일을 그 프로세스의 주소 공간 중 일부로 메모리 사상해 준다.  
파일이 open(), write(), read()같은 일반적인 시스템 호출로 접근 된다고 해도  
Solaris는 모든 파일을 커널 주소 공간이기는 하지만 역시 메모리 사상시킨다.  
즉, 파일이 어떻게 open되든지 간에 Solaris는 모든 파일의 입출력을 
궁극적으로 모두 메모리 사상으로 취급하여 파일 접근이 효율적인 메모리 관리 시스템을 통해 이루어지도록 한다.  


여러 프로세스들이 데이터 공유를 위래 이러한 파일을 공유하는 수도 있다.  
이러한 경우 한 프로세스가 공유 중인 메모리 사상 파일에 write를 하면  
그 write는 즉시 다른 모든 프로세스들도 볼 수 있게 된다.  
지금까지 본 장에서 기술한 가상 메모리를 이해하면 이러한 공유가 어떻게 가능한지를 알 수 있다.  
그 파일을 공유하는 프로세스들의 page mapping table은 모두 그 파일에 대응하는 물리 메모리 상의 페이지를 가리킨다.  
이 페이지에는 기스크 파일의 내용이 올라와 있다.  
한편 메모리 사상 관련 시스템 호출들이 copy-on-write기능을 지원하여  
파일을 read-only로 공유할 때에는 모든 프로세스들이 한 개의 페이지를 공유하다가,  
어떤 프로세스가 그것을 수정하기 시작하면 그 시점에 그 프로세스를 위해 별도의 페이지 복사본을 만들어 줄  
수도 있다. 이 때 페이지 수정 작업이 프로세스들 가에 동기화되어야 한다면 6장에서 기술한 상호 배제 기법을 함께 사용하면 된다.  


사실 공유 메모리를 메모리 사상 파일을 이용하여 구현하는 것은 매우 자주 있는 일이다.  
이러한 방식에서는 동일한 파일을 프로세스들의 가상 주소 공간에 메모리 사상하게 함으로써  
프로세스들은 공유 메모리를 사용하여 통신할 수 있다.  
메모리 사상된 파일은 통신하는 프로세스들 사이의 공유 메모리 영역으로 동작한다.  
우리는 이미 POSIX공유 메모리 객체를 생성하고 각 통신 프로세스가 이 객체를 자신의 주소 공간에 메모리 사상하는 것을 보았다.  
이후 절에서 메모리 사상 파일을 이용하는 공유 메모리를 지원하기 위한 Windows API에 대해서 설명한다.  

###쓰레싱(Thrashing)
만약 어떤 프로세스에게 할당된 프레임 수가 명령어 집합 아키텍처(instruction set architecture)가  
요구하는 최소한의 수보다 적게 되면, 그 프로세스는 실행을 한동안 일시 중단(suspend) 하여야 한다.  
그 프로세스의 모든 페이지를 포기하고 다른 프로세스들에게 넘겨주어야 한다.  
이러한 상황을 위해 스왑-인, 스왑-아웃 단위의 CPU스케줄링이 필요함을 알 수 있다.  


"충분한" 프레임을 할당받지 못한 프로세스에 대해 생각해 보자.  
활발하게 사용되는 페이지 집합을 지원해 줄 만큼 프레임을 충분하게 할당받지 프로세스는  
페이지 부재가 바로 발생할 것이다. 이 때 페이지 교체가 필요하지만 이미 활발하게 사용되는  
페이지들만으로 이루어져 있으므로 어떤 페이지가 교체되든 바로 다시 필요해질 것이다.  
결과적으로 바로바로 반복해서 페이지 부재가 발생하며 교체된 페이지는  
다시 얼마 지나지 않아 일어올 필요가 생긴다.  


이러한 과도한 페이징 작업을 스레싱(thrashing)이라고 부른다.  
어떤 프로세스가 실제 실행보다 더 많은 시간을 페이징에 사용하고 있을 경우 스레싱이 발생했다고 한다.  
###스레싱의 원인(Cause of Thrashing)
스레싱은 심각한 성능저하를 초래한다. 초기의 페이징 시스템에서 실제로 있었던 다음 경우를 보자.  


운영체제는 CPU의 이용률(utilization)을 감시한다.  
만약에 CPU이용률이 너무 낮아지면 새로운 프로세스를 시스템에 추가해서 다중 프로그래밍의 정도  
(degree of multi-programming)를 높인다. 이 때 전역 페이지 교체 알고리즘을 사용하여 
어떤 프로세스의 페이지인지에 대한 고려 없이 교체를 수행한다.  
이제 어떤 프로세스가 새로운 실행 단계로 진입하여 더 많은 프레임을 필요로 한다고 가정하자.  
페이지 부재가 발생하면서 다른 프로세스로부터 프레임들을 가져오게 될 것이다.  
그런데 교체된 페이지들이 해당 프로세스에서 필요로 하는 것이었다면,  
그 프로세스 역시 페이지 부재를 발생시키고 또 다른 프로세스에서 프레임을 가져온다.  
이러한 프로세스들이 페이지 스왑 인, 스왑 아웃을 위해 페이징 장치를 사용해야 한다.  
페이징 장치에 대한 큐잉이 진행되면서 ready큐는 비게 된다.  
프로세스들이 페이징 장치를 기다리는 동안 CPU이용률은 떨어진다.  


CPU스케줄러는 이용률이 떨어지는 것을 보고, 이용률을 높이기 위하여 새로운 프로세스를 추가하여  
다중 프로그래밍의 정도를 더 높인다. 새로 시작하는 프로세스는 실행중인 프로세스 들로부터  
프레임을 더 가져오고자 하며, 더 많은 페이지 부재와 더 긴 페이징 장치 대기 시간을 야기 한다.  
결과적으로 CPU이용률은 더욱 떨어지고 CPU스케줄러는 다중 프로그래밍 정도를 더욱 높이려고 한다.  
결국 스레싱이 일어나게 되어 시스템의 처리율은 대단히 낮아지고 페이지 부재는 상당히 늘어난다.  
실질 메모리 접근 시간은 증가하고 프로세스들은 페이징하는 데 시간을 다 소비하게 되어  
결국 아무런 일도 할 수 없게 된다.  


처음에는 다중 프로그래밍의 정도가 높아짐에 따라서 CPU의 이용률도 높아진다.  
증가 속도가 감소하기는 하지만, 최대값에 도달하기 까지 증가한다.  
그러나 다중 프로그래밍의 정도가 그 이상으로 더 커지면 스레싱이 일어나게 되고  
CPU의 이용률은 급격하게 떨어지게 된다. 따라서 이 지점에서는 CPU이용률을 높이고 스레싱을  
중지시키기 위해 다중 프로그래밍 정도를 낮춰야만 한다.  


보충 필요


다중 프로그래밍:
다중 프로그래밍(Multi-programming)이란 CPU작업과 입출력 작업을 병행하는 것이다.  
CPU이용과 처리량을 향상시킬 수 있다.  
###버퍼와 캐시
버퍼는 한 곳에서 다른 곳으로 데이터를 이동할때 그 데이터를 이동하기 위해 사용되는 
물리적인 메모리 저장소의 영역을 뜻한다.  


두 개 이상의 소프트웨어 또는 하드웨어의 입출력을 결합하는데 이용된다.  
예를 들어서 A, B라는 별개의 소프트웨어 또는 하드웨어가 있고,  
이 때 A에서는 작업의 결과물로서 어떤 데이터를 내놓으며 B에서는 어떤 데이터를 입력받아  
작업을 한다고 가정해보자. 여기서 데이터를 내놓는다는 것은  
특정 메모리 영역에 데이터를 기록한다는 것을 의미하며,  
데이터를 입력 받는다는 것은 특정 메모리 영역의 데이터를 읽는다는 것을 의미한다. 
이러한 상황에서 A의 결과물로 나온 데이터를 B에 입력하고자 하는 것이 목적일 경우,  
A의 출력 데이터를 특정 메모리 영역 'α'에 저장하고 B에서 특정 메모리 영역 'α'의 데이터를  
읽게 하는 일련의 과정을 거쳐서 목적을 달성 하게 된다.  
이 때 여기서 특정 메모리 영역 'α'가 버퍼에 해당하게 된다.  


캐시 메모리, 스풀 또는 스트리밍 등과 그 의미를 혼동하는 경우가 종종 있다.  
버퍼와 캐시는 동작과 사용에서 차이가 존재한다.  
버퍼는 궁극적으로 이동하고자 하는 데이터를 모두 담게 되지만,  
캐시는 설계에 따라서 데이터를 전부 담을 수도 있고 일부를 담을 수도 있으며  
아예 담지 않을 수도 있다. 버퍼를 이용하는 경우에는,  
원래 데이터가 들어있는 장소의 데이터를 버퍼로 이동한 후 버퍼를 이용하므로  
명시적으로 버퍼를 이용하는 구조이다. 하지만 캐시를 사용하는 경우에는,  
원래 데이터가 들어있는 장소를 이용할 때 실제로는 캐시가 그 장소 대신 이용되는 구조이다. 
그리고 그 캐시가 원래 데이터가 있던 장소를 이용하게 할 것인지 캐시에 있는 공간을  
이용하게 할 것인지를 결정하게 된다.  
버퍼를 이용하는 경우에는 버퍼에 들어오지 않은 데이터는 즉시 이용할 수 없는 반면에,  
캐시를 이용하는 경우에는 원래 데이터의 모든 부분을 캐시를 통해서 즉시 이용할 수 있다.  


스풀 또는 온라인 비디오 스트리밍은 버퍼를 이용해서 구현된다.  
즉, 버퍼를 이용하는 방식 중에 하나이다. 따라서 비동기적 작업을 하거나  
받은 데이터만을 이용해서 작업할 수 있다는 특성 등은 버퍼를 특정 방식으로  
응용함으로서 발생한 특성인 것일 뿐, 버퍼 자체의 특성은 아니다.  


-출처 나무위키: 버퍼
###가상 메모리의 장점

가상 메모리의 장점은 크게 2가지로 나뉘는데, 바로 성능측면에서 장점과  
보안 측면에서의 장점 2가지로 나뉜다. 하지만 이런 장점을 누릴 수 있는 이유는 확실히 구분되지 않는다.  
차근차근 알아보도록 하자.  

####성능
가상 메모리를 사용함으로써 얻는 성능상 이점은 *보다 많은 메모리 자원*을 사용할 수 있다는 점,  
그리고 프로세서의 가용률(보다 많은 프로세스를 동시에 올려놓고 태스크를 처리할 수 있다.)을 높여서  
*단위 시간당 더 많은 프로세스의 자원*을 사용할 수 있다는 점 대표적으로 2가지가 있다.  


메모리 자원:  
가상 메모리를 사용한다. -> 잘 생각해보면, 프로세스 스케줄러에 올라와 있는 모든 프로세스의  
전체 프로세스 이미지를 *모두* 올릴 필요는 없다. 왜 그럴까? 이유를 하나씩 알아보자.

1. 100 \* 100 배열이 할당되어 있어도 프로세스가 돌아가는 대부분의 시간동안 10 \* 10 부분만 집중적으로 참조될 수 있다.  

2. 프로그램의 코드 중, 정말 어쩌다 한번 돌아가는 코드가 있기는 하지만 죽은 코드(Dead Code)는 절대 아닌 그런 코드가 있다.

3. 프로세스 이미지 전체를 메모리에 올려놓고 할 일을 처리해야 하는 프로세스가 있다고 하자, 모든 프로세스가 이런 프로세스는 아니며 그렇다고 해도 모든 자원을 **동시에** 요구하는 것은 아니다.  

즉, 각각의 프로세스에게 가상 주소 공간을 넘겨주고 모든 프로세스가 자신에게 필요한 데이터를 전부 가상 주소 공간에 올려놓으면,  
모든 프로세스는 실제 물리적인 램에 당장 데이터가 없어도, 자신에게 필요한 데이터가 전부 있다고 착각에 빠지게된다.  
그리고 가상 메모리를 구현하는 요구 페이징 기법을 사용하여 *필요할 때* 가상 주소를 통해 메모리에 있는 데이터를 요청하고  
물리 주소에 매핑되어 있으면 가져오고 없으면 페이지 부재(page fault)를 발생 시켜서 페이지를 (이 과정에서 페이지 교체가 일어날 수 있다.)  
스왑-인하여 데이터를 가져온다. 이 과정을 도와주는 하드웨어가 바로 MMU와 TLB이다.  


잠깐 삼천포로 빠졌지만 결론을 말하자면 가상 메모리 기법을 사용하면 실제 물리 주소 공간보다 훨씬 큰 메모리 공간을  
사용할 수 있게 되고, 다시 말하자면 보다 효율적으로 메모리를 사용할 수 있게된다.  

프로세서 자원:  
가상 메모리 기법을 사용하여 얻는 이점은 메모리 공간을 효율적으로 사용하는 것 뿐만 아니라,  
프로세서 자원도 보다 효율적으로 사용할 수 있게 되는데, 이는 다음과 같은 이유 때문이다.  

1. 각 사용자 프로그램이 더 작은 메모리를 차지하므로 더 많은 프로그램을 동시에 수행할 수 있게 된다.

2. 프로그램을 메모리에 올리고 스왑(swap)하는 데 필요한 입출력 회수가 줄어들기 때문에 프로그램들이 보다 빨리 실행된다.  

더 작은 메모리를 사용하게 되면 동시에 더 많은 프로세스를 프로세스 스케줄러 위에 올릴 수 있게된다.  
따라서, 응답 시간(response time, turn around time)은 늘어나지 않으면서도(프로세스 상태가 변하지는 않는다.)  
CPU이용률(utilazation)과 처리율(throughput)이 높아진다. (동시에 더 많은 프로세스를 스케줄러 위에 올린다.)  


또한 프로세스 이미지 전체를 물리 메모리 위에 올렸다 내렸다(swap-in, swap-out)하는 것이 아니라,  
각 프로세스 마다 필요한 만큼만 배정을 받고 프로세스가 실행됨에 따라서 필요한 부분을 요구하여  
부분적으로 교체 및 할당 받기 때문에 시스템이 입출력에 필요한 자원을 아껴서 프로세스에게 줄 수 있게 된다.  

####보안과 안정성
가상 메모리를 사용하는 시스템 위에서 돌아가는 프로세스는 한 가지 큰 착각에 빠져있다. 뭘까?  
그것은 바로 자신이 시스템의 모든 메모리를 전부 다 사용하고 있다는 착각이다.
즉, 프로세스는 자신만의 가상 주소공간을 각자 배정받아 사용하고 있고, 그 크기는 해당 운영체제가 접근 가능한  
주소의 최대 크기가 된다. 예를 들어, 32비트 운영체제면 4GB, 64비트 운영체제이면 **16EB**만큼 할당 받았다고 보면 된다.  
(물론 커널 영역도 배정되어 있지만 해당 영역을 커널이 아닌 프로세스가 접근하면 커널이 프로세스를 바로 죽여버린다.)  
이 사실이 시사하는 것은 굉장히 크다. 다시 말하지만, 각각의 프로세스 마다 **독립된 가상 주소 공간**을 가져서 얻는 이득이 또 있는데,  
다른 프로세스들의 간섭으로부터 보호받을 수 있게된다.(물론 커널에게는 모든 정보가 있다.)  
또한 프로세스가 잘못 동작해도 독립적인 가상 주소 공간으로 보호받고 있으므로, 대체로 문제를 해당 프로세스 선에서 마무리 지을수 있다.  
(다시 말해, 프로세스가 오류를 일으켜도 다른 프로세스에 해를 입히지 않는다.   
 물론 IPC코드가 오동작 하거나 해당 프로세스가 루트 권한을 가진다면 이야기가 달라지지만 아주 일반적인 상황은 아니다.)    
악의적인 사용자가 프로세스를 장악하는데 성공했다고 해도, 프로세스 내부의 가상 주소만 접근 할 수 있기때문에  
할 수 있는 일이 크게 제한되며 시스템을 공격하는 등의 행동을 하려고 해도, 여러 단계를 거쳐야 된다.  


또한 프로세스에는 각각의 독립적인 메모리 레이아웃이 있는데, 이를 프로세스 이미지(Process image)라고 한다.  
프로세스 이미지는 그 얼개가 프로그램에 의해서 결정된다. 설계도가 같으면 똑같이 생긴 건물이 나온다는 뜻이다.  
그리고 같은 프로그램으로 부터 만들어진 프로세스라 할지라도, 프로세스의 상태에 따라서 이미지 내부의 값은 각각 다를 수 있다. 
이는 건물이 똑같이 생겼어도 사는 사람은 다를 수 있다는 뜻이다.  


위에서도 이야기 했듯이 프로세스 이미지는 세그먼트라고 하는 단위로 구분되어 있다. 각각의 세그먼트는 경계를 가지고 있는데,   
이 세그먼트의 경계를 넘어서는 메모리에 접근하려고 하면 또는 읽기 전용 세그먼트에 쓰기를 시도하는 등 잘못된 접근을 시도하면,   
커널이 **Segmentation fault**를 표준 에러로 내보내면서 프로세스를 강제 종료한다.  
이렇게 가상 메모리를 사용하면 성능 뿐만 아니라 보안도 강화된다.  
(참고로 세그먼트 경계검사는 하드웨어에서 지원하는 기능이다.)  


##그리디 알고리즘
탐욕 알고리즘이라고도 부르는 이 알고리즘의 특징은,  
말 그대로 알고리즘이 매번 해를 구하기 위한 선택의 기로에 놓일 때마다,  
최대한 욕심을 부려서 해를 선택한다. 즉, 선택할 수 있는 여러개의 대안이 있다면 이 때 욕심을 부린다는 뜻이다.  
여기서 욕심을 부린다는 말을 다른 말로 풀어보면, 근시안적으로 의사 결정을 내린다는 뜻으로도 볼 수 있다.  


문제를 풀 때 문제의 전체적인 상황을 고려하거나 혹은 앞뒤의 상황을 고려하면 전체적인 관점에서 최적의 해답을 찾을 수 있다.  
하지만 문제가 어렵거나 복잡하면, 상황을 고려하는데 많은 시간과 노력이 소모된다.   
또는 전체 상황을 고려하는 것 자체가 너무 어려워서 쉽지 않을 수 있다.  
그러나 탐욕 알고리즘은 전체적인 관점에서 문제를 해결하려고 하지 않는다.  
당장 눈앞에 닥친 상황만 보고 결정을 내린다. 따라서 근시안적인 의사결정을 할 수 밖에 없게 되고,  
이것이 이 알고리즘의 이름이 '탐욕 알고리즘'인 이유다.  


요약하자면 다음과 같다.  
* 탐욕 -> 욕심을 부리다. -> 눈 앞의 상황만으로 결정을 내린다.  


어렵고 복잡한 문제의 가장 좋은 답을 최적해(Optimal Solution)라고 한다.  
다른 알고리즘들은 최적해를 구하기 위해서 어렵고 복잡한 방법을 이용한다.  
계산이 어렵고 복잡할수록 문제를 푸는데 시간이 더 걸릴 수 있다.  
하지만 최적해를 찾기 위해서 기꺼이 시간을 더 사용한다.  


반면 탐욕 알고리즘은 굳이 최적해를 구하려고 하지 않는다.  
적당히 **괜찮은 답**(물론 우연히 최적해일수도 있다.)을 찾으려고 한다.  
따라서 문제 전체를 고려하려 하지도 않고 기존 판단을 번복하지도 않으며,  
수학적으로 증명된 몇가지 경우를 제외하고는 최적해를 구하지 않는다.  

* 탐욕 알고리즘의 개념: 각 단계에서 최선의 선택을 하고 이들의 집합이 해가 된다.
* 장점 : 빠른 계산
* 단점 : 최적해를 보장하지 않는다.  

###기본 구성 요소와 특징
탐욕 알고리즘은 단계마다 한 번 선택한 답은 이후에 변경하지 않는다.  
즉, 각 단계의 답은 별개로, 나중에 선택한 답이 이전에 선택한 답에 영향을 주지 않는다.  
예를 들어, 노드 B에서 선택한 결과가 이미 결정을 내린 노드A의 결과에 영향을 미치지 않는다.  
즉, 답의 확장성을 포기하는 대신 빠른 계산을 선택한다.  


그러면 탐욕 알고리즘은 주로 어떤 경우에 사용하는가?  
앞서 탐욕 알고리즘은 근시안적 판단을 하므로 최적해를 구하지 못할 수 있다고 하였다.  
반면, 그렇기 때문에 계산이 단순하고 시간이 적게 걸린다는 장점이 있다.  
따라서 최적해를 구하기 무척 어려운 문제라는 판단이 든다면,  
오히려 탐욕 알고리즘을 사용하는 게 비교적 괜찮을 해를 적당히 빠른 시간에 얻을 수 있다는 점도 장점이 된다.  


이제 탐욕 알고리즘의 가장 기본적인 요소를 정리해 보자.  

* 선택 함수(Selection function): 현재 단계에서 최선의 후보를 선택  

* 가능성 점검 함수(Feasibility function): 선택된 후보가 답이 될 수 있는지 제약 사항 점검  

* 해 판단 함수(Solution function): 지금까지 선택으로 문제의 해가 완성되었는지 점검

탐욕 알고리즘은 단계별로 '선택 함수'를 이용하여 최적이라고 예상되는 후보를 만들어 낸다.  
여러 개의 해답 후보가 있을 수 있다. 따라서 가능한 후보 중에서 현재 상황에서 가장 우수하다고 예상되는 해답을 골라내야 한다.  
그리고 이 후보가 해답으로 가능한지 '가능성 점검 함수'를 이용하여 검증한다. 문제별로 제약 사항이 있을 수 있다.  
아무리 좋은 선택이라고 하더라도 제약 사항을 어긴다면 답이 될 수 없기 때문이다.  


탐욕 알고리즘이 만드는 답은 단계별 선택의 집합이라고 했다.  
따라서 선택을 하다가 어느 시점이 되면 자연스럽게 하나의 답이 완성된다.  
'해 판단 함수'는 지금까지의 선택들로 문제의 답(해)이 만들어졌는지 점검한다.  
만약, 답이 만들어졌다면 알고리즘을 종료해야 한다.  
이러한 구성요소를 이용하여 문제를 해결하는 탐욕 알고리즘의 과정을 의사코드로 정리하면 다음과 같다.  

```{.c}
// 탐욕 알고리즘의 의사코드
GreedyAlgorithm( Input ){
	for( Input의 모든 요소 e ){
		s = SelectionFunction( e );
		if( FeasibilityFunction( s ) ){
			Solution <- s추가;
			
			if( SolutionFunction( Solution ) )
				return Solution;
		}
	}
	return Solution;
}
```
탐욕 알고리즘으로 분류되는 알고리즘의 수도 다양하다.  
여기서는 탐욕 알고리즘의 대표적인 몇가지 예를 들어보도록 하겠다.  

###동전 거스름돈 문제
동전 거스름돈 문제(Coin change problem)는 최소한의 동전을 이용하여 거스름돈을 만드는 방법을 찾는 문제를 말한다.  
거스름돈을 만들더라도 가능한 한 동전의 개수가 적으면 적을수록 좋다. 간단한 예를 살펴보자.  
예를 들어 10원, 50원, 100원, 500원의 동전이 있다고 가정해 보자.  
이 때, 거스름돈으로 170원을 어떻게 만들면 좋을까? 다음 표를 보면 3가지 경우가 있을 수 있다.  


| 해답 후보들                  | 동전 개수   |
|------------------------------|-------------|
| 10 \* 17                     | 17개        |
| 50 \* 3 + 10 \* 2            | 5개         |
| 100 \* 1 + 50 \* 1 + 10 \* 2 | 4개(최적해) |


최적해를 구한 경우처럼 최소한의 동전으로 거스름돈을 만드는 경우를 구하려면 어떤 방법을 사용해야 할까?  
바로 이 문제에 탐욕 알고리즘을 적용해 보도록 하겠다.  


먼저, 어떤 '기준'으로 동전을 선택할까? 사실 이 문제는 간단한데, 평소에 우리가 거스름돈을 어떻게 주는지 생각해보면 된다.  
가장 액수가 큰 동전으로 거슬러 줄 수 없는 경우에는 그 다음으로 작은 단위 동전을 이용하는 것이다.  
동전 중에서는 500원이 가장 크지만, 500원은 거슬러 주어야 할 170원보다 크기 때문에 제외해야 한다.  
그렇다면 그 다음으로 큰 동전인 100원으로 계산해 볼 수 있다.  


이처럼 탐욕 알고리즘은 우리의 직관적 의사 결정을 그대로 옮긴 것으로 해석할 수 있다.  
지금 까지의 과정을 의사코드로 정리해보겠다.  

* 동전 거스름돈 문제에 탐욕 알고리즘의 적용

	* 큰 동전부터 선택

	* 현재 선택한 동전을 최대한 사용한다.  
```{.c}
// change_total: 거스름돈의 전체 금액
// coins: 동전들의 정보
GreedyAlgorithm_CoinChange( change_total, coins ){
	SortCoins( coins )						// 동전의 단위가 큰 순서대로 coins를 정렬
	for( Coins의 모든 요소 e ){
		s = SelectionFunction( e )			// 동전 e의 최대 개수를 선택한다.
		Solution <- s 추가					// 기존 해에 동전 e의 개수를 추가한다.
		if( SolutionFunction( Solution ))	// 남은 잔돈이 없으면 종료한다.
			return Solution;
	}
	return Solution;
}
```
이 알고리즘이 일반적인 탐욕 알고리즘과 다른 점은 동전을 큰 순서대로 먼저 정렬한다는 점이다.  
바로 이 부분이 이번 탐욕 알고리즘에서 가장 중요한 부분이다.  


그 다음 부터는 일반적인 탐욕 알고리즘의 흐름을 그대로 따라가고 있다.  
각 동전 e별로 최대 개수를 선택하여 기존 해인 Solution에 추가해 준다.  
예를 들어, 100원 짜리 동전을 선택한다면 최대 개수인 1개를 추가한다.  
다만, 여기서 선택된 후보가 답이 될 수 있는지 제약 사항을 점검하는 가능성 점검 함수는 제외되었다.  
그 이유는 선택 함수 SelectionFunction()자체가 이미 제약 사항을 고려하여 계산하기 때문에 굳이 가능성을 점검할 필요가 없기 때문이다.  
또한 이렇게 부분 해를 추가한 다음에 해 판단 함수 SolutionFunction을 호출하여  
지금까지의 선택으로 문제의 해가 완성되었는지 점검한다. 이 경우 남은 잔돈이 0원이 되었다면 알고리즘은 종료한다.  
이제 이 의사코드를 실제 C코드로 구현해 보겠다.  

####동전 거스름돈 문제는 항상 최적해를 구하는가?
앞서 말했듯이 수학적으로 증명된 몇가지 경우를 제외하면 탐욕 알고리즘은 언제나 최적해를 찾는 것은 아니라고 했는데,  
실제 화폐 시스템에서는 대부분 탐욕 알고리즘을 이용해 최적해를 찾는다.  
하지만 동전 거스름돈 알고리즘은 반드시 최적해를 찾는다고 보장하는 알고리즘이 아니다. 다음은 반례다.  


예: 거스름돈 140원  
동전의 종류: 100원, 70원, 50원, 10원  
최적해: 2개 (70원 \* 2개)
탐욕 알고리즘: 5개 (100원 \* 1개, 10원 \* 4개)  


그렇다면 항상 최적해를 구하려면 어떻게 해야 할까?  
이 문제는 8장에서 다룰 동적 계획법을 이용하면 항상 최적해를 구할 수 있다.  
8장에서 이 문제를 다시 한 번 살펴보겠다.  


####탐욕 알고리즘의 구현
주요 소스 파일은 따로 폴더를 배정해서 자세한 주석과 함께 만들었다.  
해당 디렉토리를 참고할 것  

###배낭 문제
배낭 문제는 배낭에 어떤 물건을 효율적으로 담을지 결정하는 문제를 말한다.  
따라서 배낭에 담긴 물건의 총 가격이 가능한 한 높도록 물건을 선택해야 한다.  
물론, 배낭에 담을 각 물건에는 무게와 가격이 매겨져 있으며 가방에 담을 수 있는 전체 무게에도 제한이 있다.  
그렇기 때문에 최대한 값비싸면서 가벼운 물건을 선택해야 한다. 예를 들어, 다음 표를 보면 물건 D는 무게가 4KG로 가장 무겁지만,  
가격은 800원으로 물건 C에 이어 두 번째로 비싸다. 따라서 물건D보다 1KG가볍고 100원 비싼 물건 C를 넣는 것이 효율적이라는 것을 알 수 있다.  


|       | 가격  | 무게     |
|-------|-------|----------|
| 배낭  |       | 5kg 한도 |
| 물건A | 500원 | 1kg      |
| 물건b | 300원 | 2kg      |
| 물건c | 900원 | 3kg      |
| 물건d | 800원 | 4kg      |


그러나 우리가 이번 장에서 탐욕 알고리즘으로 살펴볼 문제는 그냥 배낭 문제가 아니라,  
부분 배낭 문제(Fractical knapsack problem)이다. 왜 이름 앞에 '부분'이라는 말이 붙어 있을까?


먼저, 부분 배낭 문제란 짐을 쪼갤 수 있는 경우에 푸는 배낭 문제를 말한다.  
예를 들어, 배낭에 넣을 수 있는 최대 무게는 5kg이다. 이 때 물건 A와 물건 B를 먼저 선택했다고 가정해 보자.  
이 때 물건 A와 물건 B를 먼저 선택했다고 가정해 보자. 물건 A와 물건 B의 무게 합은 3KG이다.  
이 경우 배낭에 아직 2kg의 여유가 남아있다. 그러나 물건 C와 물건 D의 무게는 각각 3KG과 4KG이라서 물건 C와 물건D를 온전히 배낭에 담을 수 없다.  
하지만, 물건을 쪼갤 수 있다면 물건의 일부분을 담을 수 있다. 예를 들어 다음 그림에서는 물건 C의 2/3만큼만 쪼개어 담은 경우를 보여준다.  
3KG인 물건 C를 2/3만큼 쪼갠 무게는 2kg이다.  


단, 물건을 쪼갰기 때문에 물건의 가치도 해당 비율만큼 쪼개진다.  
따라서 물건 C의 가격인 900원을 2/3만큼 쪼갠 600원이 배낭에 포함된다.  
부분 배낭 문제는 이처럼 물건을 쪼갤 수 있는 경우에 짐의 가치가 최대가 되도록 물건을 선택하는 문제이다.  


물건을 쪼갤 수 있다고 가정한 부분 배낭 문제가 있다면, 짐을 쪼갤 수 없다고 가정하는 배낭문제도 있을 것이다.  
짐을 쪼갤 수 없는 배낭 문제를 0-1 배낭 문제(0-1 knapsack problem)라고 한다.  
여기서 0은 짐을 싣지 않는다는 뜻으로 보면 되고, 1은 짐을 싣는다는 뜻으로 보면 된다.  
예를 들어 앞의 배낭 문제를 0-1 배낭 문제에 적용하면 물건 A와 물건 B를 담은 뒤에는 더 이상 물건을 선택할 수 없다.  


참고로, 부분 배낭 문제는 탐욕 알고리즘으로 최적해를 구할 수 있는 반면,  
0-1 배낭 문제는 탐욕 알고리즘으로 최적해를 구할 수 없다.  
0-1 배낭 문제에 대한 최적해를 구하는 방법은 8장의 동적 계획법에서 다시 다루도록 한다.  

#### 탐욕 알고리즘의 적용

배낭 문제는 기본적으로 어떤 물건을 얼마만큼 담을 것인지 결정해야 한다.  
따라서 어떤 기준으로 물건들을 선택할지와 선택한 물건을 얼마만큼 담을 것인지가 알고리즘의 핵심 내용이 된다.  
이런 면에서 이 문제는 앞절에서 살펴본 동전 거스름돈 문제와 유사하다.  
동전 거스름돈 문제에서는 '금액'을 기준으로 동전의 종류를 선택하고 선택한 동전을 최대한 사용했다.  


그러면 부분 배낭 문제에서는 어떤 기준으로 물건을 골라야 할까?  
무게가 무거운 물건을 먼저 골라야 할까? 가격이 비싼 물건을 골라야 할까?  
배낭에 담을 수 있는 물건의 무게가 정해져 있기 때문에 단순히 가격이 비싼 물건을 고르면 안된다.  
또한, 배낭에 담을 물건의 가격이 가장 높아야 하기 때문에 단순히 무게가 무거운 물건을 골라도 안된다.  
결론적으로 단위 무게당 가격이 가장 비싼 물건을 골라야 한다. 즉, (가격/무게)의 값이 가장 큰 물건부터 담아야 한다.  
그리고 가격-무게 비율을 기준으로 일단 물건을 골랐다면 담을 수 있을 만큼 최대한 담으면 된다.  


요약:  


* (가격/무게)가 큰 물건부터 선택  

* 현재 선택한 물건을 최대한 담는다.  

이제 감이 오는가? 탐욕 알고리즘의 핵심은 **선정 기준**을 어떻게 잡는가?  
바로 이것이다.  

#### 의사코드 적용

탐욕 알고리즘을 통해서 배낭 문제를 해경하는 과정을 의사코드로 작성해 보면 다음과 같다.  

```{.c}
// 탐욕 알고리즘의 의사코드
// capacity: 배낭의 용량
// items: 물건의 정보(무게, 가격)

GreedyAlgorithm_FractionKnapsack( capacity, items ){
	Sort_items( items );			// (가격/무게) 비율이 큰 순서대로 items를 정렬
	for( items의 모든 요소 ){
		s = SelectionFunction( e );	// 물건 e를 최대한 선택
		Solution <- s 추가;			// 기존 해에 물건 e를 추가한다.
		if( SolutionFuction( Solution )) // 배낭에 공간이 더 없으면 종료한다.
			return Solution;
	}
	return Solution;
}
```

### 허프만 코딩
탐욕 알고리즘과 관련하여 마지막으로 허프만 코딩(Huffman coding)알고리즘에 대해서 배워보겠다.  
(허프만 코딩을 이해하기 위한 압축 관련 용어들이 있는데, 잘 알지 못한다면 잠깐 아래쪽에 갔다올 것)  


압축을 위한 허프만 코딩의 핵심 개념은 '기호의 빈도수'에 따른 '가변 길이 코딩'으로 요약할 수 있다.  
여기서 '기호의 빈도수'와 '가변 길이 코딩'이라는 조금 어려워 보이는 단어가 나온다.  
사실 개념 자체는 쉽기 때문에 간단한 예를 따라가다 보면 자연스럽게 이해될 것이다.  
일단 다음과 같이 알파벳으로 만든 문자열 ABCDABCABA를 허프만 코딩을 이용하여 압축해 보겠다.  


먼저, 기호의 빈도수는 문자 기호(알파벳)가 몇 번 나오는가를 가리킨다.  
예를 들어 위 문자열에서는 A는 4번 나오고 D는 1번만 나온다.  
이러한 압축 대상 문자열에서 각 기호의 빈도수를 구하면 다음 표와 같다.  


표


허프만 코딩은 빈도수가 큰 문자와 작은 문자를 처리하는 방식이 **다르다.**
빈도수에 따라 어떻게 처리하는지 가변 길이 코딩에 대해서 살펴보도록 한다.  
'가변 길이 코드(Variable length coding)'가 있다면 '고정 길이 코드(Fixed length code)'도 있을것이다.  
사실 압축 분야를 제외하면 우리가 사용하는 대부분의 분야에서는 '고정 길이 코드'를 사용한다.  
보통 '코드'라고 하면 대부분 이 '고정 길이 코드'를 말한다.  
왜냐하면, 고정 길이 코드를 사용하는 것이 가변 길이 코드를 사용하는 것보다 훨씬 편리하기 때문이다.  


가장 대표적인 고정 길이 코드로는 아스키 코드(ASCII, American Standard Code for Information Interchange)를 들 수 있다.  
앞서 알파벳 A에 해당하는 숫자가 65라고 했듯이 문자를 숫자 65로 바꾼 이유는 아스키 코드에서 정의한 값이기 때문이다.  


아스키 코드는 길이가 8비트(bit)로 고정되어 있다.  
즉, 아스키 코드에서 글자 1개는 무고전 8비트다.  
따라서 2진수로 표기해 보면 00000000에서 01111111까지 8개의 비트로 표현된다.  
이러한 고정 길이 코드를 주로 사용하는 이유는 사용이 편리하기 때문이다.  
특히 디코딩이 무척 편리하다. 숫자의 길이가 고정되어 있기 때문에 고정된 길이만큼만 읽어들이면 되기 때문이다.  


아스키 코드는 8비트 크기 이므로, 8비트를 읽어들여서 해당 문자가 A라는 것을 파악했다.  
이 문자열의 다음 문자도 마찬가지 방법으로 8비트씩 읽어들여서 해당 문자로 디코딩할 것이다.  
그러나 고정 길이 코드는 저장 공간을 낭비한가는 단점이 있다.  
왜냐하면, 자주 사용하는 문자의 길이의 길이를 짧게 하면 저장 공간이 훨씬 줄어들기 때문이다.  
물론 가끔 사용하는 문자의 길이가 더 길어질 수도 있다.  
하지만 자주 사용하는 문자의 길이가 짧아지기 때문에 전체 문자열의 길이는 줄어들 것이다.  


요약: 고정 길이 코드의 장점과 단점

* 장점: 사용이 편리하다.

* 단점: 저장 공간의 낭비가 있다.


이제 '기호의 빈도수'와 '가변 길이 코딩'이 어떤 관계인지 느낌이 오는가?  
압축이란 자료가 저장되는 공간을 줄이는 작업을 말한다.  
따라서 허프만 코딩은 자주 나오는 문자는 길이를 짧게 하고 드물게 나오는 문자는 길이를 길게 한다.  
그리고 문자를 이러한 가변 길이 코드로 코딩하면 전체 문자열을 저장하는 데 필요한 저장 공간이 절약 된다.  


예를 들어 가변 길이 코딩 방법을 적용하면,  
문자 A의 길이는 1비트이지만, 문자 B는 2비트로, 길이가 서로 다르다는 것을 알 수 있다.  
만약 고정 길이코드로 저장했다면 16비트가 필요했겠지만, 가변 길이 코드로 저장할 때는 3비트면 충분하다.  
저장 공간이 상당히 절약되었다는 것을 알 수 있다.  


코딩된 문자열을 실제 사용하려면 원래의 문자열로 디코딩해야 한다.  
다만 문자마다 길이가 다르기 때문에 고정 길이 코드보다 좀 더 복잡한 과정을 거쳐야 한다.  
따라서 가변 길이 코드의 단점은(고정 길이 코드보다)사용이 불편하다는 것이다.  


요약: 가변 길이 코드의 장점과 단점  

* 장점: 저장 공간이 절약된다.  

* 단점: 디코딩 과정에서 사용이 불편하다.  

이제 허프만 코딩 알고리즘을 살펴보기 전 마지막으로 허프만 코딩 알고리즘을 이해하는 데 필요한 개념인  
접두어 코드(prefix code)를 살펴보도록한다. 앞서 허프만 코딩은 압축하려는 문자열을 입력 파라미터로 전달 받아 가변 길이 코드로 바꾸는 것이라고 배웠다.  
또한, 이 과정에서 자주 사용하는 글자의 길이는 짧게 한다는 것도 배웠다.  


그러나 여기서 주의할 제약 사항이 있다. 바로 코드(숫자 값)가 서로 겹치지 않아야 한다는 것이다.  
먼저 간단한 고정 길이의 코드의 예를 가지고 살펴보도록 하자. 예를 들어 아스키 코드를 보면 글자 A의 값은 65다.  
그렇다면 다른 글자는 숫자 65를 사용할 수 없다. 왜냐하면, 숫자 값이 서로 달라야 글자를 구분할 수 있기 때문이다.  
즉, 한 글자의 값과 다른 글자의 값이 같아서는 안 된다.  


가변 길이 코드에서도 '코드가 서로 겹치지 않아야 한다.'는 제약 사항은 여전히 필요하다.  
왜냐하면, 코드가 겹치면 서로 다른 글자를 구분할 수 없기 때문이다. 그럼 이러한 제약 사항을 어떻게 적용하나?  
일단 간단한 예를 가지고 가변 길이 코드에서의 제약 사항이 적용되는 과정을 살펴보겠다.  

* A -> 1

* B -> 01

* 010... <- 맨 앞의 비트는 A인가? 아니면 B의 시작인가?

만약 디코딩해야 할 문자열이 010...으로 시작한다면 처음 0을 만났을 때 우리는 이 코드가 문자 A의 코드인지 아니면 문자B의 시작 코드인지 알 수 없다.  
왜냐하면, 코드가 서로 겹쳤기 때문이다.  
접두어 코드는 가변 길이 코드의 한 종류로, '한 코드가 다른 코드의 접두어가 되지 않는 코드'를 말한다.  
즉, 이런 경우와 같이 A의 코드 0이 다른 코드 B의 접두어(시작)가 되어서는 안 된다.  
따라서 이런 경우는 다음과 같이 바뀌면 접두어 코드가 된다.  

* A -> 0

* B -> 10

* 0(A)10(B)..

다음은 이러한 접두어 코드 및 접두어 코드가 되지 않는 예를 보여준다.  


표


이제 허프만 코딩 알고리즘을 이해하는 데 필요한 개념들은 모두 살펴보았다.  
지금까지의 내용을 정리해 보면 다음과 같다. 압축 대상이 되는 문자열에서는 문자의 빈도수에 따라 접두어 코드를 만들 수 있다.  
이렇게 만들어진 접두어 코드로 문자열을 코딩하면 고정 길이 코드를 사용하는 것보다 훨씬 효율적으로 저장 공간을 사용할 수 있다.  


#### 탐욕 알고리즘의 적용

##### 허프만 트리의 생성

##### 허프만 트리 VS 허프만 코드 테이블


#### 허프만 코딩을 이해하기 위한 압축 관련 용어들
허프만 코딩(Huffman coding)을 제대로 이해하기 위해서는 이 알고리즘이 적용되는 분야에 대한 약간의 지식이 필요하다.  
허프만 코딩은 이를 개발한 데이비드 허프만(David Huffman)의 이름을 딴 알고리즘으로, 압축 알고리즘 중에서 가장 대표적인 알고리즘이다.  
즉, 알고리즘의 적용 분야가 '압축' 분야이다.  


참고로, 우리가 많이 사용하는 ZIP 압출 알고리즘(ZIP's Compression algorithm)도 허프만 코딩을 기반으로 구현되었다.  
압축 방식에도 다양한 방식이 있는데, 허프만 코딩은 특별히 무손실 압축(Lossless compression)방식으로 분류된다.  
여기서 무손실이란 원래 자료의 훼손 없이 압축된다는 뜻이다. 예를 들어서, 우리가 ZIP으로 압축된 파일을 풀었을 때  
손실 없이 원본 파일을 그대로 사용할 수 있다. 따라서 자료의 정확성이 필요한 분야에서는 이러한 무손실 압축을 사용한다.  
반면, 손실 압축(Loss compression)은 압축을 풀었을 때 자료 일부가 훼손된 압축 방식이다.  
보통 인터넷 스트리밍 서비스 등과 같이 속도가 중요한 분야에서 사용하는 방식이다.  


그렇다면 알고리즘 이름에 코딩(Coding)이 들어간 이유는 무엇일까?  
압축 분야에서 코딩이란 '코드(code)로 바꾼다.'는 뜻이다.  
여기서 코드란 숫자를 말한다.즉, 원래의 문자 기호를 숫자 기호로 바꾸는 작업을 말한다.  
예를 들어, 다음과 같이 알파벳 A와 같은 문자 정보를 65와 같은 숫자로 바꾸는 것을 말한다.  
코딩을 한국어로는 부호화라고도 하는데, A와 같은 문자 정보를 65와 같은 숫자 '부호'로 만든다는 뜻이 된다.  


  ->(코딩coding)  
A				 	B  
  (디코딩decoding)<-  


그럼 숫자 정보를 원래 문자 정보로 바꾸는 것은 무엇일까?  
그림을 보면 알 수 있듯이 숫자를 이용하여 원래의 문자 기호로 바꾸는 것은 디코딩이라고 한다.  
즉, 숫자 65를 원래의 문자 기호 A로 바꾸는 경우다. 디코딩을 한국어로는 복호화라고도 한다.  


참고로 프로그램 분야에서 말하는 '코딩'은 프로그램을 만드는 작업으로 소스 코드(Source code)를 만드는 과정을 말한다.  
따라서 문자를 숫자로 바꾸는 압축에서 말하는 코딩과는 다소 다른 개념이다.  














							   

