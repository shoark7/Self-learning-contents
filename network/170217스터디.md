#메모리 관리

##가상 메모리가 필요한 이유, 그리고 장점
가상 메모리는 프로세스 전체가 메모리(즉, 계층 구조상 메인 메모리 이상)내에   
올라오지 않더라도 실행이 가능하도록 하는 기법이다.  
이 기법의 주요한 장점중 하나는 컴퓨터 위에서 돌아가고 있는 프로세스들의 메모리 크기가  
실제 *물리 메모리보다 더 커져도 된다*는 점이다.  
또한 *파일의 공유*를 쉽게 해주고, *공유 메모리 구현*을 가능하게 한다.  
추가적으로 *프로세스 생성을 효율적*(COW)으로 처리할 수 있는 기법도 제공한다.  


좀 더 구체적으로 가상 메모리의 필요성에 대해서 알아보자.  
프로그램을 실행하기 위해서는 모든 코드가 메모리에 존재하는 상태여야 한다고 생각할 수도 있다.  
물론, 동적 적재(Dynamic loading -> 나중에 설명할 것이다.)를 사용해서  
전체 프로세스 메모리를 올리지 않아도 되도록 코드를 작성할 수도 있지만  
이는 운영체제의 도움을 거의 받을 수 없는 방법이다.  


다시 본론으로 돌아와서 생각해보자. 실행중인 코드 전체가 반드시 메모리에 있어야 된다면,  
- 이는 타당해보일지 몰라도 - 프로그램의 크기를 물리 메모리의 크기로 제한할수 밖에 없게된다.  
그러나 잠시만 생각해보아도 프로그램 전체가 한꺼번에 메모리에 올라올 필요가 없다는 사실도 알 수 있다.  


* 프로그램의 오류 처리 코드를 생각해보자. 오류는 거의 발생하지 않으므로,  
해당 코드는 거의 실행되지 않는다.
* 배열, 리스트, 테이블 등 여러가지 자료구조가 필요 이상으로 많은 공간을 점유할 수도 있다.  
예를 들어 100 X 100만큼 선언된 배열이 실제로는 10 X 10만큼만 사용될 수도 있다.
* 반드시 필요하지만 거의 사용되지 않는 코드도 있다.  
예를 들어 매일 사용하는 프로그램일지라도 1달에 한번만 해당 루틴이 필요할 수도 있다.


이런 이유로 대부분의 프로그램은 메모리 위로 한꺼번에 모든 코드가 올라올 필요가 없다.  
또한, 전체 프로그램이 필요한 경우에도 그 프로그램의 모든 부분이 모두 동시에 요구되지 않을 수 있다.  
위에서도 이야기 했지만, 프로그램의 일부분만 메모리에 올려놓고 실행할 수 있다면 다음과 같은 많은 이점이 있다.  

* 물리 메모리의 크기 제약이 크게 완화된다.  
또한 메모리를 매우 거대한 배열로 간주하는 추상화를 제공하므로, 프로그래밍이 쉬워진다.  
* 응용 프로그램들이 한번에 더 작은 메모리를 차지하므로,
더 많은 프로그램을 동시에 수행할 수 있게 된다. 따라서 CPU의 이용율과 처리율이 늘어난다.
* 더 적은 메모리를 사용하게 되면 프로그램을 메모리에 올리고 스왑(Swap)하는데  
필요한 입출력 회수가 줄어들기 때문에 프로그램이 보다 빠르게 실행된다.


실질적으로 보다 큰 메모리를 사용할 수 있는 장점 말고 다른 장점도 있다.  
논리 메모리를 물리 메모리로부터 분리시켜주는 것 외에 가상 메모리는 페이지 공유를 통해  
파일이나 메모리가 둘 또는 그 이상의 프로세스들에 의해 공유되는 것을 가능하게 한다.  
이는 다음과 같은 장점이 있다.  

* 시스템 라이브러리가 여러 프로세스들에게 공유될 수 있다.  
각 프로세스들은 라이브러리가** 자신의 가상 주소 공간의 일부**라고 생각하지만,  
실제로는 라이브러리가 존재하는 물리 메모리 페이지들은 모든 프로세스에게 공유되고 있다.  
* 마찬가지로, 프로세스들이 메모리를 공유할 수 있다.  
둘 또는 그 이상의 프로세스가 공유 메모리를 통해 통신할 수 있다.  
가상 메모리는 한 프로세스가 다른 프로세스와 공유할 수 있는 영역을 만들 수 있도록 해 준다.  
이 영역을 공유하고 있는 프로세스들에는 각자 자신의 주소 공간상에 있는 것처럼 보이지만,  
실제 물리 메모리는 공유되고 있다.  
* 페이지는 fork() 시스템 호출을 통한 프로세스 생성 과정 중에 공유될 수 있기 때문에  
프로세스 생성 속도를 높일 수 있다.  

##요구 페이징
요구 페이징(demand paging)은 가상 메모리의 구현 방식이며  
세그멘테이션 방식으로 구현 될 수도 있으나,  
대개는 하나의 세그먼트가 다시 여러 페이지로 나뉘는 페이지된 세그멘테이션 기법을 사용한다.  


이를 프로세스 내의 개별 페이지 수준에서 관리해주는 것을 페이저(pager)라고 한다.
요구 페이징 기법은 어떤 점에서는 스와핑(swapping)과 비슷하지만,
요구 페이징과 관련해서는 스와핑, 스와퍼보다는 페이저라는 용어를 사용한다.  


페이저는 swap in시에 프로세스가 다시 swap out되기 전에  
실제로 사용될 페이지들이 어떤 것인지 추측한다.  
따라서 페이저는 프로세스 전체를 스왑 인 하는 대신에 실제 필요한 페이지들만 메모리로 읽어 온다.
이는 낭비를 줄일 수 있는 효과적인 방법이다.  


이 방법을 사용하려면 약간의 하드웨어 지원이 필요하고, 
유효-무효 비트기법이 여기서 사용될 수 있다. 
페이지 테이블 내의 페이지의 유효-무효 비트가 켜저(1)있다면 메모리 내부에 있는것이고,  
꺼져(0)있다면 가상 주소 공간 상에 정의되어 있지 않거나,  
유효하지만 디스크(보조 기억장치)에 존재한다는 것을 의미한다.  


이 때 프로세스가 꺼져있는 비트를 가진 페이지에 접근하려고 하면  
페이지 부재 트랩(page-fault trap)을 발생시킨다. 처리 과정은 그림을 보라. 
요구 페이징을 극단적으로 사용하면 메모리에 페이지가 하나도 올라오지 않은 상태에서도  
프로세스를 실행시킬 수 있다. 이런 방식을 *순수 요구 페이징(pure demand paging)*이라고 한다.


그러면 여기서 이런 의문을 가질 수도 있다. "요구 페이징은 성능 저하를 일으키지 않을까요?"
그러나 성능 저하는 생각보다 매우 작다. 모든 프로그램은 참조의 지역성(locality of reference)를 가진다.  
프로그램은 한동안 특정할 수 있는 작은 부분만 집중적으로 참조하는데  
이러한 성질 때문에 요구 페이징은 만족할 성능을 보인다.  
(그러나 구현이 쉬운 것이 결코 아니다.) 


스왑 인:  
보조기억장치에 있는 데이터를 메모리로 읽어들이는 행위  


스왑 아웃:
메모리에 있는 데이터를 보조기억장치에 쓰는 행위


스와핑:
위와 같이 보조기억장치와 주 메모리 사이에서 데이터를 주고받는 행위를 총칭하는 단어  
##쓰기 시 복사(Copy-on-write)
앞에서 페이지를 공유 함으로써 프로세스를 빠르게 생성할 수 있다는 요지의 말을 했는데,  
이번에는 그 부분에 대해서 알아보자.  
알다시피 fork()시스템 콜은 부모 프로세스와 똑같은 자식 프로세스를 만들어준다.  
하지만 자식 프로세스는 곧바로 exex()을 호출해서 부모로부터 복사해온 페이지들을 다 쓸모없게 만든다.  
그래서 fork()할 때 부모 프로세스의 페이지를 몽땅 복사해오는 대신에 쓰기 시 복사(COW)방식을 사용할 수 있다.  


이 방식은 자식 프로세스가 시작할 때 부모의 페이지를 당분간 함께 사용하도록 한다.  
이 때 공유되는 페이지를 쓰기 시 복사(copy-on-write)페이지라고 표시한다.  
"둘 중 한 프로세스가 공유 중인 페이지에 쓸 때 그 페이지의 복사본이 만들어 진다"는 의미이다.  
이렇게 되면, 자식 프로세스는 수정이 일어나는 페이지만 복사하여,  
수정되지 않는 페이지들에 대해서는 계속 공유 될 수 있게 한다.  

##페이지 교체 + 교체 알고리즘
페이지 부재에 대해서 말하지 않은 것이 있다.  
각 페이지는 처음 그 페이지가 접근 될 때 한번만 페이지 부재가 발생하는 것처럼 말했는데,  
이런 설명이 정확한 것이 아니다. 만약 페이지를 전체 10개 사용하는 프로그램이 
실제로는 5개의 페이지만 사용한다고 해보자, 하나의 프로그램에서 비교적 많은 프로세스가 실행 가능한 것은 맞지만,  
무한하게 실행 가능한 것은 아니다.(가상 메모리도 유한하다.)  


여기에 더해서 시스템 메모리는 프로그램 페이지만 저장하는 용도로만 사용되는 것이 아니다.  
입출력을 위한 버퍼도 상당한 양의 메모리를 사용하고, 이는 메모리 할당 알고리즘의 부담을 키운다.  
어느 정도의 메모리를 입출력 용도로 할당하고,  
또 어느 정도로 프로그램에 할당할 것인지 결정하는 것은 매우 중요한 문제다.  


여기서 운영체제는 몇 가지 선택을 할 수 있는데, 일단 사용자 프로세스를 종료할 수 있다.  
하지만 요구 페이징은 운영체제가 더 효율적인 사용자 경험을 위해서 선택한 방법이므로,  
위와 같은 선택은 가장 좋은 선택이라고 보기 어렵다.  
또한 사용자가 운영체제가 페이징 시스템에서 실행되고 있음을 알아서도 안된다.
대신 운영체제는 프로세스 하나를 swap out하여 해당 프로세스의 프레임들을 해제하고,
다중 프로그래밍[3] 정도를 낮출 수 있다. 어떤 환경에서 이 선택은 훌륭한 것이다.  


즉, 중요도가 떨어지는 페이지는 swap out하고 더 중요한 페이지를 디스크에서 swap in한다.
이를 페이지 교체(page replacement)라고 한다. 페이지 교체 과정에 대해서는 그림을 참고하라.  


페이지 교체는 요구 페이징의 기본이다. 이를 통해 논리적 메모리와 물리 메모리간의 분리가 완성된다.  
요구 페이징을 하지 않더라도 사용자 주소가 물리 주소로 사상되어 서로 다른 주소 집합을 가질 수 있다.  
그러나 이런 방식으로 구현하면 프로세스의 모든 페이지는 물리 메모리에 존재해야만 한다.  
요구 페이징은 논리 주소 공간의 크기가 물리 메모리에 의한 제약에서 벗어나도록 해준다.  
어떤 프로세스가 20개의 페이지로 이루어져 있으면, 10 프레임만 가지고도  
요구 페이징과 필요할 때 마다 빈 페이지를 찾아주는 교체 정책을 이용해 이 프로세스를 실행 할 수 있다.  
교체 되려는 페이지가 변경이 된 경우에는 디스크에 복사된다.  
나중에 이 페이지를 참조하게 되면 페이지 부재가 발생한다.  
그 때 페이지는 메모리로 다시 돌아오게 되며, 아마도 프로세스의 다른 페이지를 교체하게 될 것이다.  


요구 페이징 시스템은 두 가지 중요한 문제를 해결해야 하는데, 
**프레임 할당(frame allocation)알고리즘** 과 **페이지 교체(page replacement)알고리즘**이다.  
여러 프로세스가 있다면, 각 프로세스에 얼마나 많은 프레임을 할당해야 할 지 결정해야 한다.  
또한 페이지 교체가 필요할 때 마다 어떤 페이지를 교체해야 할지 결정해야 한다.  
디스크 입출력이 매우 많은 비용을 요구하는 것을 생각해 보면,  
이런 문제를 해결할 수 있는 적절한 알고리즘을 설계하는 것은 중요한 일이다.  
요구 페이징 방법은 조금만 개선해도 시스템의 전체 성능이 크게 향상 될 수 있다.  
이제 가장 유명한 페이지 교체 알고리즘 부터 하나씩 알아보자.  

1. FIFO 페이지 교체
가장 간단한 페이지 교체 알고리짐은 FIFO(first-in first-out)알고리즘이다.  
FIFO 교체 알고리즘은 어떤 페이지를 교체해야 할 때, 메모리에 올라온지 가장 오래된 페이지를 내쫓는다.  
페이지가 올라온 시간을 페이지 마다 기록하는 방법도 좋고,  
아니면 페이지들이 올라온 순서로 큐를(FIFO Queue) 만들어 가지고 있어도 된다.  


이 교체 알고리즘은 이해하기도 쉽고, 프로그램 하기도 쉽다.  
하지만 성능이 항상 좋지는 않다. 교체된 페이지가 더이상 사용되지 않는 초기화 모듈일 수도 있지만,  
반대로 교체된 페이지가 초기화 된 뒤 계속해서 자주 사용하는 변수를 포함하고 있을 수도 있다.  


실제로 이 알고리즘을 채택한 뒤 페이지 부재율을 관찰해 보면,
프레임을 더 줄수록 오히려 페이지 부재율이 증가한다는 사실을 알 수 있다.  
이런 결과는 상식 밖의 것이면, 이러한 현상을 **Belady의 모순**이라고 한다.  

2. 최적 페이지 교체
FIFO가 Belady의 모순으로 인해 별로 좋은 알고리즘이 아님이 밝혀지자,  
연구자들은 최적 교체 정책을 탐색하기 시작했다. 결과적으로 그런 정책은 존재 했고 OPT 또는 MIN으로 불렸다.  
이 정책을 요약하면 다음과 같다.  


**앞으로 가장 오랫동안 사용되지 않을 페이지를 찾아 교체하라**


실제로 (구현되었다고 가정하고)FIFO에 비해서 훨씬 좋은 성능을 보인다.
하지만 안타깝게도 이 알고리즘의 실제 구현은 어렵다.  
이 방식은 프로세스가 앞으로 메모리를 어떻게 참조할 것인지를 미리 알아야하기 때문이다.  
이 알고리즘이 사용되는 경우는 주로 비교 연구 목적을 위해서 사용한다.  

3. LRU(Least recently used \ 최근 최소 사용) 페이지 교체
최적 알고리즘이 불가능 하다면 최적 알고리즘의 근사 알고리즘은 가능할 것이다.  
FIFO와 OPT알고리즘의 결정적인 차이는 FIFO알고리즘이 페이지가 메모리로 들어온 시간을 이용하는데 비해,  
OPT알고리즘은 페이지가 사용될 시간을 이용한다는 것이다.  
최근의 과거를 가까운 미래의 근사치로 본다면, 가장 오랜 기간 동안 사용되지 않은 페이지를 교체할 수 있다.  
이 기법이 least-recently-used(LRU)알고리즘이다.  


이 기법은 각 페이지마다 마지막 사용 시간을 유지한다.  
페이지 교체시에 LRU는 가장 오랫동안 사용되지 않은 페이지를 선택한다.  
이 정책은 미래 대신 과거 시간에 대해 적용한 최적 교체 정책으로 생각할 수 있다.  


성능은 최적 교체 알고리즘보다는 못하지만, FIFO보다는 좋다.  
따라서 LRU정책은 페이지 교체 알고리즘으로 자주 사용되며 좋은 알고리즘으로 인정받고 있다.  
문제는 *어떻게* 이 알고리즘을 구현하느냐 하는 것이다.  
LRU 페이지 교체 알고리즘은 하드웨어의 지원이 필요하다.  
페이지들을 최근 사용된 시간 순서로 파악하는 것을 하드웨어가 지원해 주어야 한다.  
두 가지 구현 방법이 가능하다.  

* 계수기(Counter)

* 스택(Stack)

하지만 양쪽 LRU 구현 방법 모두 반드시 표준적인 TLB 레지스터 이상의 하드웨어 지원이 있어야 한다.  
계수기 값이나 스택을 갱신하는 일을 메모리 참조 때마다 수행되어야 한다.  
이정도 메모리 관리 오버헤드를 감당할 수 있는 시스템은 거의 없다.  

4. LRU 근사 페이지 교체
LRU 페이지 교체 지원을 충분히 할 수 있는 하드웨어는 거의 없다.  
그러나 많은 시스템은 **참조 비트**(reference bit)의 형태로 어느 정도의 지원은 하고 있다.
처음에는 모든 참조 비트는 운영체제에 의해 0으로 채워진다.  
페이지 참조가 있을 때마다(페이지 내의 어떤 바이트라도 읽거나 쓸 때) 하드웨어가 1로 세팅한다.  
어느 정도 지나면 페이지 사용의 *순서*는 모르지만, 어떤 페이지가 그동안 사용되었고,  
어떤 페이지가 한번도 사용되지 않았는지를 알 수 있다.  
이런 부분적인 정보가 많은 LRU 근사 알고리즘의 기본이 된다.  

* 부가적 참조 비트 알고리즘
일정한 간격마다 참조 비트들을 기록함으로써 추가적인 선후 관계 정보를 얻을 수 있다.  
각 페이지에 대해 8비트의 참조 비트를 할당한다.  
일정한 시간 간격마다, 예를 들면 매 100ms마다, 타이머 인터럽트(timer interrupt)를 걸어서  
운영체제가 참조 비트를 8비트 정보의 최상위 비트로 이동시키고,  
나머지 비트들은 하나씩 오른쪽으로 이동시킨다.  
8비트 시프트 레지스터(shift register)는 가장 최근 8구간 동안의 그 페이지의 사용 기록을 담고 있다.  


예를 들어 시프트 레지스터 값이 00000000이라면 페이지를 8번의 구간동안 한번도 사용하지 않았다는 뜻이고,  
각 구간 마다 최소한 한번 이상 사용된 페이지는 11111111의 시프트 레지스터 값을 가진다.  
이 8비트 값을 정수로 생각하면 가장 작은 수를 갖는 페이지가 LRU페이지가 되고 이를 교체할 수 있다.  
가장 작은 값을 갖는 페이지 모두를 교체할 수도 있고 그들 사이에서 FIFO방식으로 하나를 선택 할 수도 있다.  


물론 사용하는 비트 수(시프트 레지스터의 크기)는 달라질 수 있고, (하드웨어의 지원 여부에 따라)
갱신을 가장 빠르게 하기 위한 크기가 선택된다.  
극단적인 경우 크기는 0(참조 비트 이외에 없음)될 수 있고, 이 경우 참조 비트만이 남게 된다.  
이 알고리즘을 **2차 기회 알고리즘**이라고 부른다.

* 2차 기회 알고리즘
2차 기회 알고리즘의 기본은 FIFO 교체 알고리즘이다.  
그러나 페이지가 선택될 때마다 참조 비트를 확인한다.  
참조 비트가 0이면 페이지를 교체하고 1이면 다시 한 번 기회를 주고 다음 FIFO페이지로 넘어간다.  
한 번 기회를 받게 되면 참조 비트는 해제되고 도착 시간이 현재 시간으로 재설정된다.  
이에 따라 그 페이지는 다른 모든 페이지들이 교체 될 때까지 (또는 기회를 받을 때 까지) 교체되지 않는다.  
따라서 참조 비트가 계속 설정되어 있을 정도로 자주 사용되는 페이지는 전혀 교체되지 않을 것이다.  


2차 기회 알고리즘을 구현하는 하나의 방법은 순환 큐(circular queue)를 이용하는 것이다.  
이 큐에는 포인터가 있어서 다음에 교체될 페이지를 가리킨다.  
희생될 페이지가 필요하다면, 포인터는 0값의 참조 비트를 가진 페이지를 발견할 때 까지 큐를 뒤진다.  
포인터가 돌아가면서 참조 비트 값들이 1인것은 0으로 바꾼다.  
희생될 페이지가 발견되면, 그 페이지는 교체되고 새로운 페이지는 순환 큐의 해당 위치에 삽입한다.  
최악의 경우, 모든 비트가 1값을 가지고 있었다면 포인터는 큐를 완전히 한바퀴 돈다.(그러면서 0값으로 세팅)  
그러고 나면 모든 페이지는 기회가 주어지게 된다. 두 번째 돌 때에는 사실상 FIFO와 같은 것이 된다.  


* 개선된 2차 기회 알고리즘
참조 비트와 변경 비트를 사용하면 더 개선시킬 수 있다.  

5. 계수-기반 페이지 교체

6. 페이지-버퍼링 알고리즘

7. Application과 페이지 교체

##프레임의 할당

1. 최소로 할당해야 할 프레임의 수

2. 할당 알고리즘

3. 전역 대 지역 할당

4. 비균등 메모리 접근

##스레싱(Thrashing)

###스레싱의 원인  

###작업 집합 모델

###페이지 부재 빈도

* 작업 집합과 페이지 부재율

##메모리 사상 파일

###기본 기법

###Windows API에서 공유 메모리

###메모리 사상 입출력

##커널 메모리의 할당

###버디 시스템

###슬랩 할당

##기타 고려 사항

###프리페이징

###페이지 크기

###TLB Reach

###역 페이지 테이블

###프로그램 구조

###입출력 상호 잠금(I/O Interlock)과 페이지 잠금(locking)



##각종 용어 정리

[1]보조 기억장치에 있는 데이터를 메모리로 불러들이는 것을 스왑 인이라 한다.

[2]메모리에 있는 데이터를 보조 기억장치에 저장하고, 메모리에서 내리는 것을 스왑 아웃이라 한다.

[3]다중 프로그래밍(Multi-programming)이란 CPU 작업과 입출력 작업을 병행하는 것이다.  
CPU 이용과 처리량을 향상시킬 수 있다.  

##참고 문헌


